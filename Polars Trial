{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\n\n# Load the entire dataset with Polars\ntrain_data = pl.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet')\n\n# Filter out the first 85 days\ntrain_data = train_data.filter(pl.col(\"date_id\") >= 85)\n\n# Drop fully empty rows (all NaN)\ntrain_data = train_data.drop_nulls(subset=train_data.columns)\n\n# Define columns to exclude\nexclude_columns = ['date_id', 'time_id', 'symbol_id', 'weight'] + \\\n                  [col for col in train_data.columns if col.startswith('responder_')]\n\n# Select numerical columns excluding non-relevant ones\nnumerical_columns = [col for col in train_data.columns if col not in exclude_columns]\n\n# Interpolate missing values for numerical columns\ntrain_data = train_data.with_columns([\n    pl.col(col).interpolate() if col in numerical_columns else pl.col(col) \n    for col in train_data.columns\n])\n\n# Drop rows with any remaining NaN\ntrain_data = train_data.drop_nulls()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T04:31:32.057818Z","iopub.execute_input":"2024-11-27T04:31:32.058577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute IQR thresholds\nq1 = train_data.select([pl.col(col).quantile(0.25).alias(col) for col in numerical_columns])\nq3 = train_data.select([pl.col(col).quantile(0.75).alias(col) for col in numerical_columns])\niqr = q3 - q1\n\n# Calculate lower and upper bounds\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\n# Clip outliers for numerical columns\ntrain_data = train_data.with_columns([\n    pl.col(col).clip(lower_bound[col].item(), upper_bound[col].item())\n    for col in numerical_columns\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport polars as pl\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# Fit the scaler on a sample dataset\nsample_data = train_data.select(numerical_columns).head(1000).to_pandas()\nscaler.fit(sample_data)\n\n# Create an empty list to store scaled columns\nscaled_columns = []\n\n# Process each column individually\nfor col in numerical_columns:\n    print(f\"Processing column: {col}\")\n\n    # Define batch size\n    batch_size = 100_000\n\n    # Process the column in batches\n    scaled_col_batches = []\n    for start in range(0, train_data.shape[0], batch_size):\n        end = start + batch_size\n\n        # Select the column batch and convert to Pandas for compatibility\n        batch = train_data[start:end].select([col]).to_pandas()\n\n        # Scale the batch\n        scaled_batch = scaler.transform(batch)\n\n        # Convert scaled batch back to Polars\n        scaled_col_batches.append(pl.Series(name=col, values=scaled_batch.flatten()))\n\n    # Concatenate scaled batches for this column\n    scaled_column = pl.concat(scaled_col_batches)\n\n    # Append to scaled columns list\n    scaled_columns.append(scaled_column)\n\n# Add scaled columns back to the original DataFrame\nfor scaled_col in scaled_columns:\n    train_data = train_data.with_column(scaled_col)\n\n# Verify the scaled data\nprint(train_data.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for columns with NaN values\nnan_counts = train_data.null_count()\n\n# Convert to dictionary of scalar values\nnan_counts_dict = {col: count[0] if isinstance(count, list) else count for col, count in nan_counts.to_dict(as_series=False).items()}\n\n# Filter columns with NaN values\ncolumns_with_nan = {col: count for col, count in nan_counts_dict.items() if count > 0}\nprint(\"Columns with NaN values:\\n\", columns_with_nan)\n\n# Check for constant columns (zero variance)\nconstant_columns = [\n    col for col in numerical_columns\n    if train_data[col].std(ddof=0) == 0  # Directly compare the result\n]\nprint(\"Constant columns:\\n\", constant_columns)\n\n# Drop constant columns\nif constant_columns:\n    train_data = train_data.drop(constant_columns)\n    numerical_columns = [col for col in numerical_columns if col not in constant_columns]\n    print(f\"Dropped {len(constant_columns)} constant columns.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to calculate and filter high correlations\ndef filter_high_correlation(data, threshold=0.9):\n    corr_matrix = data.corr()\n    correlated_pairs = [(i, j) for i in range(len(corr_matrix)) for j in range(i + 1, len(corr_matrix)) \n                        if abs(corr_matrix.iloc[i, j]) > threshold]\n    return correlated_pairs\n\n# Apply on a sample or smaller subset\ncorrelated_pairs = filter_high_correlation(train_data.sample(fraction=0.1, with_replacement=False).to_pandas())\nprint(\"Highly correlated pairs:\", correlated_pairs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}