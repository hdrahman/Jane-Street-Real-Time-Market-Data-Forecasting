{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\n\n# Load the entire dataset with Polars\ntrain_data = pl.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet')\n\n# Filter out the first 85 days\ntrain_data = train_data.filter(pl.col(\"date_id\") >= 85)\n\n# Drop fully empty rows (all NaN)\ntrain_data = train_data.drop_nulls(subset=train_data.columns)\n\n# Define columns to exclude\nexclude_columns = ['date_id', 'time_id', 'symbol_id', 'weight'] + \\\n                  [col for col in train_data.columns if col.startswith('responder_')]\n\n# Select numerical columns excluding non-relevant ones\nnumerical_columns = [col for col in train_data.columns if col not in exclude_columns]\n\n# Interpolate missing values for numerical columns\ntrain_data = train_data.with_columns([\n    pl.col(col).interpolate() if col in numerical_columns else pl.col(col) \n    for col in train_data.columns\n])\n\n# Drop rows with any remaining NaN\ntrain_data = train_data.drop_nulls()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:41:40.408455Z","iopub.execute_input":"2024-11-27T03:41:40.409514Z","iopub.status.idle":"2024-11-27T03:42:34.940657Z","shell.execute_reply.started":"2024-11-27T03:41:40.409440Z","shell.execute_reply":"2024-11-27T03:42:34.939430Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Compute IQR thresholds\nq1 = train_data.select([pl.col(col).quantile(0.25).alias(col) for col in numerical_columns])\nq3 = train_data.select([pl.col(col).quantile(0.75).alias(col) for col in numerical_columns])\niqr = q3 - q1\n\n# Calculate lower and upper bounds\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\n# Clip outliers for numerical columns\ntrain_data = train_data.with_columns([\n    pl.col(col).clip(lower_bound[col].item(), upper_bound[col].item())\n    for col in numerical_columns\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:42:34.942626Z","iopub.execute_input":"2024-11-27T03:42:34.943069Z","iopub.status.idle":"2024-11-27T03:44:58.084817Z","shell.execute_reply.started":"2024-11-27T03:42:34.943019Z","shell.execute_reply":"2024-11-27T03:44:58.083568Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport polars as pl\n\n# Initialize MinMaxScaler and set output to Polars\nscaler = MinMaxScaler(feature_range=(0, 1)).set_output(transform=\"polars\")\n\n# Define a batch size\nbatch_size = 100_000  # Adjust based on available memory\n\n# Get total number of rows\nnum_rows = train_data.shape[0]\n\n# Create a new DataFrame to store scaled results\nscaled_train_data = []\n\nfor start in range(0, num_rows, batch_size):\n    end = start + batch_size\n    \n    # Select a batch\n    batch = train_data[start:end].select(numerical_columns)\n    \n    # Scale the batch\n    scaled_batch = scaler.fit_transform(batch)\n    \n    # Append the scaled batch to the result list\n    scaled_train_data.append(scaled_batch)\n\n# Concatenate all scaled batches into a single DataFrame\nscaled_train_data = pl.concat(scaled_train_data)\n\n# Replace the numerical columns with scaled values in the original DataFrame\ntrain_data = train_data.with_columns(scaled_train_data)\n\n# Verify the scaled data\nprint(train_data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:44:58.086412Z","iopub.execute_input":"2024-11-27T03:44:58.087298Z","iopub.status.idle":"2024-11-27T03:45:01.162198Z","shell.execute_reply.started":"2024-11-27T03:44:58.087244Z","shell.execute_reply":"2024-11-27T03:45:01.160636Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m batch \u001b[38;5;241m=\u001b[39m train_data[start:end]\u001b[38;5;241m.\u001b[39mselect(numerical_columns)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Scale the batch\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m scaled_batch \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Append the scaled batch to the result list\u001b[39;00m\n\u001b[1;32m     26\u001b[0m scaled_train_data\u001b[38;5;241m.\u001b[39mappend(scaled_batch)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:148\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m         _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrap_data_with_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_to_wrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:122\u001b[0m, in \u001b[0;36m_wrap_data_with_container\u001b[0;34m(method, data_to_wrap, original_input, estimator)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_data_with_container\u001b[39m(method, data_to_wrap, original_input, estimator):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrap output with container based on an estimator's or global config.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        DataFrame.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     output_config \u001b[38;5;241m=\u001b[39m \u001b[43m_get_output_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _auto_wrap_is_configured(estimator):\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data_to_wrap\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:90\u001b[0m, in \u001b[0;36m_get_output_config\u001b[0;34m(method, estimator)\u001b[0m\n\u001b[1;32m     87\u001b[0m     dense_config \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dense_config \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput config must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m: dense_config}\n","\u001b[0;31mValueError\u001b[0m: output config must be 'default' or 'pandas' got polars"],"ename":"ValueError","evalue":"output config must be 'default' or 'pandas' got polars","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# Check for columns with NaN values\nnan_counts = train_data.null_count()\n\n# Convert to dictionary of scalar values\nnan_counts_dict = {col: count[0] if isinstance(count, list) else count for col, count in nan_counts.to_dict(as_series=False).items()}\n\n# Filter columns with NaN values\ncolumns_with_nan = {col: count for col, count in nan_counts_dict.items() if count > 0}\nprint(\"Columns with NaN values:\\n\", columns_with_nan)\n\n# Check for constant columns (zero variance)\nconstant_columns = [\n    col for col in numerical_columns\n    if train_data[col].std(ddof=0) == 0  # Directly compare the result\n]\nprint(\"Constant columns:\\n\", constant_columns)\n\n# Drop constant columns\nif constant_columns:\n    train_data = train_data.drop(constant_columns)\n    numerical_columns = [col for col in numerical_columns if col not in constant_columns]\n    print(f\"Dropped {len(constant_columns)} constant columns.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to calculate and filter high correlations\ndef filter_high_correlation(data, threshold=0.9):\n    corr_matrix = data.corr()\n    correlated_pairs = [(i, j) for i in range(len(corr_matrix)) for j in range(i + 1, len(corr_matrix)) \n                        if abs(corr_matrix.iloc[i, j]) > threshold]\n    return correlated_pairs\n\n# Apply on a sample or smaller subset\ncorrelated_pairs = filter_high_correlation(train_data.sample(fraction=0.1, with_replacement=False).to_pandas())\nprint(\"Highly correlated pairs:\", correlated_pairs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}