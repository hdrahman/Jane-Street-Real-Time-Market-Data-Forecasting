{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\n\n# Load the entire dataset with Polars\ntrain_data = pl.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet')\n\n# Filter out the first 85 days\ntrain_data = train_data.filter(pl.col(\"date_id\") >= 85)\n\n# Drop fully empty rows (all NaN)\ntrain_data = train_data.drop_nulls(subset=train_data.columns)\n\n# Define columns to exclude\nexclude_columns = ['date_id', 'time_id', 'symbol_id', 'weight'] + \\\n                  [col for col in train_data.columns if col.startswith('responder_')]\n\n# Select numerical columns excluding non-relevant ones\nnumerical_columns = [col for col in train_data.columns if col not in exclude_columns]\n\n# Interpolate missing values for numerical columns\ntrain_data = train_data.with_columns([\n    pl.col(col).interpolate() if col in numerical_columns else pl.col(col) \n    for col in train_data.columns\n])\n\n# Drop rows with any remaining NaN\ntrain_data = train_data.drop_nulls()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T04:31:32.057818Z","iopub.execute_input":"2024-11-27T04:31:32.058577Z","iopub.status.idle":"2024-11-27T04:32:28.926917Z","shell.execute_reply.started":"2024-11-27T04:31:32.058505Z","shell.execute_reply":"2024-11-27T04:32:28.925581Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Compute IQR thresholds\nq1 = train_data.select([pl.col(col).quantile(0.25).alias(col) for col in numerical_columns])\nq3 = train_data.select([pl.col(col).quantile(0.75).alias(col) for col in numerical_columns])\niqr = q3 - q1\n\n# Calculate lower and upper bounds\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\n# Clip outliers for numerical columns\ntrain_data = train_data.with_columns([\n    pl.col(col).clip(lower_bound[col].item(), upper_bound[col].item())\n    for col in numerical_columns\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T04:32:28.929063Z","iopub.execute_input":"2024-11-27T04:32:28.929525Z","iopub.status.idle":"2024-11-27T04:34:52.771046Z","shell.execute_reply.started":"2024-11-27T04:32:28.929455Z","shell.execute_reply":"2024-11-27T04:34:52.769712Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport polars as pl\n\n# Define a batch size for memory efficiency\nbatch_size = 100_000\n\n# Create a list to store scaled columns\nscaled_columns = []\n\n# Process each numerical column independently\nfor col in numerical_columns:\n    print(f\"Processing column: {col}\")\n    \n    # Initialize a new MinMaxScaler for the current column\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    \n    # Fit the scaler on the full column (or a sample)\n    column_sample = train_data[col].head(1000).to_pandas().values.reshape(-1, 1)  # Ensure 2D shape\n    scaler.fit(column_sample)\n    \n    # Scale the column in batches\n    scaled_batches = []\n    for start in range(0, train_data.shape[0], batch_size):\n        end = start + batch_size\n        \n        # Select a batch for the column\n        batch = train_data[start:end].select([col]).to_pandas().values.reshape(-1, 1)\n        \n        # Transform the batch (scaling)\n        scaled_batch = scaler.transform(batch)\n        \n        # Convert scaled batch back to Polars Series\n        scaled_batches.append(pl.Series(name=col, values=scaled_batch.flatten()))\n    \n    # Concatenate scaled batches for the column\n    scaled_column = pl.concat(scaled_batches)\n    scaled_columns.append(scaled_column)\n\n# Add scaled columns back to the original DataFrame\ntrain_data = train_data.with_columns(scaled_columns)\n\n# Verify scaled data\nprint(train_data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T04:46:53.549924Z","iopub.execute_input":"2024-11-27T04:46:53.550585Z","iopub.status.idle":"2024-11-27T04:47:45.611417Z","shell.execute_reply.started":"2024-11-27T04:46:53.550543Z","shell.execute_reply":"2024-11-27T04:47:45.610194Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Processing column: feature_00\nProcessing column: feature_01\nProcessing column: feature_02\nProcessing column: feature_03\nProcessing column: feature_04\nProcessing column: feature_05\nProcessing column: feature_06\nProcessing column: feature_07\nProcessing column: feature_08\nProcessing column: feature_09\nProcessing column: feature_10\nProcessing column: feature_11\nProcessing column: feature_12\nProcessing column: feature_13\nProcessing column: feature_14\nProcessing column: feature_15\nProcessing column: feature_16\nProcessing column: feature_17\nProcessing column: feature_18\nProcessing column: feature_19\nProcessing column: feature_20\nProcessing column: feature_21\nProcessing column: feature_22\nProcessing column: feature_23\nProcessing column: feature_24\nProcessing column: feature_25\nProcessing column: feature_26\nProcessing column: feature_27\nProcessing column: feature_28\nProcessing column: feature_29\nProcessing column: feature_30\nProcessing column: feature_31\nProcessing column: feature_32\nProcessing column: feature_33\nProcessing column: feature_34\nProcessing column: feature_35\nProcessing column: feature_36\nProcessing column: feature_37\nProcessing column: feature_38\nProcessing column: feature_39\nProcessing column: feature_40\nProcessing column: feature_41\nProcessing column: feature_42\nProcessing column: feature_43\nProcessing column: feature_44\nProcessing column: feature_45\nProcessing column: feature_46\nProcessing column: feature_47\nProcessing column: feature_48\nProcessing column: feature_49\nProcessing column: feature_50\nProcessing column: feature_51\nProcessing column: feature_52\nProcessing column: feature_53\nProcessing column: feature_54\nProcessing column: feature_55\nProcessing column: feature_56\nProcessing column: feature_57\nProcessing column: feature_58\nProcessing column: feature_59\nProcessing column: feature_60\nProcessing column: feature_61\nProcessing column: feature_62\nProcessing column: feature_63\nProcessing column: feature_64\nProcessing column: feature_65\nProcessing column: feature_66\nProcessing column: feature_67\nProcessing column: feature_68\nProcessing column: feature_69\nProcessing column: feature_70\nProcessing column: feature_71\nProcessing column: feature_72\nProcessing column: feature_73\nProcessing column: feature_74\nProcessing column: feature_75\nProcessing column: feature_76\nProcessing column: feature_77\nProcessing column: feature_78\nProcessing column: partition_id\nshape: (5, 93)\n┌─────────┬─────────┬───────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n│ date_id ┆ time_id ┆ symbol_id ┆ weight   ┆ … ┆ responder_ ┆ responder_ ┆ responder_ ┆ partition_ │\n│ ---     ┆ ---     ┆ ---       ┆ ---      ┆   ┆ 6          ┆ 7          ┆ 8          ┆ id         │\n│ i16     ┆ i16     ┆ i8        ┆ f32      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n│         ┆         ┆           ┆          ┆   ┆ f32        ┆ f32        ┆ f32        ┆ f64        │\n╞═════════╪═════════╪═══════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n│ 528     ┆ 68      ┆ 1         ┆ 3.886676 ┆ … ┆ 0.277614   ┆ 0.728218   ┆ 0.500043   ┆ 0.0        │\n│ 528     ┆ 68      ┆ 2         ┆ 1.396185 ┆ … ┆ 4.198513   ┆ -2.719773  ┆ 1.256736   ┆ 0.0        │\n│ 528     ┆ 68      ┆ 3         ┆ 0.664355 ┆ … ┆ 1.342476   ┆ 1.096603   ┆ 0.379095   ┆ 0.0        │\n│ 528     ┆ 68      ┆ 5         ┆ 1.960854 ┆ … ┆ 0.122081   ┆ 0.575971   ┆ -0.249354  ┆ 0.0        │\n│ 528     ┆ 68      ┆ 7         ┆ 2.161128 ┆ … ┆ -1.10497   ┆ -0.486434  ┆ 0.881457   ┆ 0.0        │\n└─────────┴─────────┴───────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for columns with NaN values\nnan_counts = train_data.null_count()\n\n# Convert to dictionary of scalar values\nnan_counts_dict = {col: count[0] if isinstance(count, list) else count for col, count in nan_counts.to_dict(as_series=False).items()}\n\n# Filter columns with NaN values\ncolumns_with_nan = {col: count for col, count in nan_counts_dict.items() if count > 0}\nprint(\"Columns with NaN values:\\n\", columns_with_nan)\n\n# Check for constant columns (zero variance)\nconstant_columns = [\n    col for col in numerical_columns\n    if train_data[col].std(ddof=0) == 0  # Directly compare the result\n]\nprint(\"Constant columns:\\n\", constant_columns)\n\n# Drop constant columns\nif constant_columns:\n    train_data = train_data.drop(constant_columns)\n    numerical_columns = [col for col in numerical_columns if col not in constant_columns]\n    print(f\"Dropped {len(constant_columns)} constant columns.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T04:48:40.222799Z","iopub.execute_input":"2024-11-27T04:48:40.224243Z","iopub.status.idle":"2024-11-27T04:48:48.522042Z","shell.execute_reply.started":"2024-11-27T04:48:40.224184Z","shell.execute_reply":"2024-11-27T04:48:48.520887Z"}},"outputs":[{"name":"stdout","text":"Columns with NaN values:\n {}\nConstant columns:\n []\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Define a function to calculate and filter high correlations\ndef filter_high_correlation(data, threshold=0.9):\n    corr_matrix = data.corr()\n    correlated_pairs = [(i, j) for i in range(len(corr_matrix)) for j in range(i + 1, len(corr_matrix)) \n                        if abs(corr_matrix.iloc[i, j]) > threshold]\n    return correlated_pairs\n\n# Apply on a sample or smaller subset\ncorrelated_pairs = filter_high_correlation(train_data.sample(fraction=0.1, with_replacement=False).to_pandas())\nprint(\"Highly correlated pairs:\", correlated_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T04:48:55.928536Z","iopub.execute_input":"2024-11-27T04:48:55.928975Z","iopub.status.idle":"2024-11-27T04:50:34.454927Z","shell.execute_reply.started":"2024-11-27T04:48:55.928932Z","shell.execute_reply":"2024-11-27T04:50:34.453589Z"}},"outputs":[{"name":"stdout","text":"Highly correlated pairs: [(0, 92), (4, 6), (4, 7), (6, 7), (16, 71), (16, 74), (18, 73), (19, 21), (25, 35), (36, 38), (36, 39), (38, 39), (51, 62), (53, 64), (77, 78), (79, 80), (81, 82)]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}