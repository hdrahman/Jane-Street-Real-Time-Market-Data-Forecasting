{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''aggregate all imports here'''\n\nimport dask.dataframe as dd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:18:23.823528Z","iopub.execute_input":"2024-11-24T23:18:23.824052Z","iopub.status.idle":"2024-11-24T23:18:27.552019Z","shell.execute_reply.started":"2024-11-24T23:18:23.824000Z","shell.execute_reply":"2024-11-24T23:18:27.550720Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"'''This is the version that drops the rows'''\n\n#Load the entire dataset with Dask\ntrain_data = dd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet')\n\n#Filter out the first 85 days\ntrain_data = train_data[train_data['date_id'] >= 85]\n#Drop fully empty (all-NaN) partitions to save memory\ndef drop_empty_partitions(df):\n    return df.dropna(how='all')\n\ntrain_data = train_data.map_partitions(drop_empty_partitions)\n\n# Define columns to exclude\nexclude_columns = ['date_id', 'time_id', 'symbol_id', 'weight'] + [col for col in train_data.columns if col.startswith('responder_')]\n\n# Select numerical columns excluding non-relevant ones\nnumerical_columns = [col for col in train_data.select_dtypes(include=['number']).columns if col not in exclude_columns]\n\n# Updated interpolate function\ndef interpolate_partition(df):\n    # Perform interpolation\n    df[numerical_columns] = df[numerical_columns].interpolate(method='linear', limit_direction='both')\n    return df\n\n# Apply the interpolation function to each partition\ntrain_data = train_data.map_partitions(interpolate_partition)\ntrain_data = train_data.dropna(how='any')\n\n#Verify if there are any remaining NaNs\n#remaining_na = train_data.isna().sum().sum().compute()\n#print(\"Remaining NaNs after interpolation:\", remaining_na)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:18:27.554284Z","iopub.execute_input":"2024-11-24T23:18:27.640615Z","iopub.status.idle":"2024-11-24T23:18:27.818082Z","shell.execute_reply.started":"2024-11-24T23:18:27.640545Z","shell.execute_reply":"2024-11-24T23:18:27.816810Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"'''Clip outliers using 1.5IQR rule'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute IQR thresholds\nq1 = train_data[numerical_columns].quantile(0.25).compute()\nq3 = train_data[numerical_columns].quantile(0.75).compute()\niqr = q3 - q1\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n#print(\"Lower bounds:\\n\", lower_bound)\n#print(\"Upper bounds:\\n\", upper_bound)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:18:27.819493Z","iopub.execute_input":"2024-11-24T23:18:27.819854Z","iopub.status.idle":"2024-11-24T23:23:26.989289Z","shell.execute_reply.started":"2024-11-24T23:18:27.819821Z","shell.execute_reply":"2024-11-24T23:23:26.987862Z"},"_kg_hide-output":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define a function to clip outliers\ndef clip_outliers(df, lower_bound, upper_bound):\n    for col in numerical_columns:\n        df[col] = df[col].clip(lower=lower_bound[col], upper=upper_bound[col])\n    return df\n\n# Apply clipping across partitions\ntrain_data = train_data.map_partitions(\n    clip_outliers,\n    lower_bound=lower_bound.to_dict(),\n    upper_bound=upper_bound.to_dict()\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:23:26.991609Z","iopub.execute_input":"2024-11-24T23:23:26.992021Z","iopub.status.idle":"2024-11-24T23:23:27.108630Z","shell.execute_reply.started":"2024-11-24T23:23:26.991973Z","shell.execute_reply":"2024-11-24T23:23:27.107481Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"'''Apply min max scaling'''\n\n#compute column-wise min and max for scaling\n\nmins = train_data[numerical_columns].min().compute()\nmax = train_data[numerical_columns].max().compute()\n\n#replace 0 with one to avoid division errors\n\nrange_values = max-mins\nrange_values[range_values ==0] = 1\n\n#Define a function to apply min-max scaling for each parittion\ndef min_max_scale_partition(df, mins, ranges):\n    for col in numerical_columns:\n        df[col] = (df[col] - mins[col]) / ranges[col]\n    return df\n\n# Apply Min-Max Scaling across partitions\ntrain_data = train_data.map_partitions(\n    min_max_scale_partition,\n    mins=mins.to_dict(),\n    ranges=range_values.to_dict()\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:23:27.110096Z","iopub.execute_input":"2024-11-24T23:23:27.110468Z","iopub.status.idle":"2024-11-24T23:28:10.824809Z","shell.execute_reply.started":"2024-11-24T23:23:27.110435Z","shell.execute_reply":"2024-11-24T23:28:10.823595Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Compute and display the summary statistics\nscaled_summary = train_data.describe().compute()\nprint(scaled_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:40:31.311116Z","iopub.execute_input":"2024-11-24T23:40:31.311690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.to_parquet('scaled_train_data.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:31:04.449028Z","iopub.execute_input":"2024-11-24T23:31:04.449736Z","iopub.status.idle":"2024-11-24T23:35:36.264103Z","shell.execute_reply.started":"2024-11-24T23:31:04.449687Z","shell.execute_reply":"2024-11-24T23:35:36.262847Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(train_data.sample(frac=0.001).compute().head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:35:36.266560Z","iopub.execute_input":"2024-11-24T23:35:36.267047Z","iopub.status.idle":"2024-11-24T23:37:55.357020Z","shell.execute_reply.started":"2024-11-24T23:35:36.266995Z","shell.execute_reply":"2024-11-24T23:37:55.355487Z"}},"outputs":[{"name":"stdout","text":"        date_id  time_id  symbol_id    weight  feature_00  feature_01  \\\n479800      530      186          2  1.456004    0.154152    0.586055   \n363983      525      294         25  1.251560    0.004017    0.408046   \n301945      522      626          3  0.834844    0.183137    0.339949   \n44261       511      758         33  1.959900    0.326834    0.315034   \n284827      522       14         25  1.259242    0.369911    0.512734   \n\n        feature_02  feature_03  feature_04  feature_05  ...  responder_0  \\\n479800    0.159091    0.270089    0.476027    0.246432  ...     0.067428   \n363983    0.300175    0.272233    0.409390    0.337950  ...    -0.083088   \n301945    0.240450    0.226411    0.463627    0.631613  ...     0.326662   \n44261     0.377439    0.376693    0.512262    0.196471  ...     0.114557   \n284827    0.309369    0.255667    0.819507    0.268377  ...     0.051329   \n\n        responder_1  responder_2  responder_3  responder_4  responder_5  \\\n479800    -0.058708    -0.122665    -0.016748     0.469916     1.578866   \n363983     0.163049     0.017835    -0.285234    -0.009877     0.098011   \n301945     0.277471     0.183069     1.039333    -0.147361     0.325159   \n44261      0.884169    -0.371751     0.216803    -1.279038     0.251147   \n284827     0.234402     0.151604     0.197262     1.822744    -1.499763   \n\n        responder_6  responder_7  responder_8  partition_id  \n479800    -0.082528     0.354043     1.740659             3  \n363983    -0.416094    -0.191083     0.241482             3  \n301945     1.064627    -0.369073     0.502840             3  \n44261      0.215799    -2.386687     0.435818             3  \n284827     0.168622     2.731123    -2.844213             3  \n\n[5 rows x 93 columns]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(train_data['responder_6'].describe().compute())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T23:37:55.358427Z","iopub.execute_input":"2024-11-24T23:37:55.358859Z","iopub.status.idle":"2024-11-24T23:40:31.308055Z","shell.execute_reply.started":"2024-11-24T23:37:55.358824Z","shell.execute_reply":"2024-11-24T23:40:31.306739Z"}},"outputs":[{"name":"stdout","text":"count    3.934201e+07\nmean    -1.517173e-03\nstd      8.860156e-01\nmin     -5.000000e+00\n25%     -2.657704e-01\n50%     -3.626317e-03\n75%      5.354323e-01\nmax      5.000000e+00\nName: responder_6, dtype: float64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"'''misc data visualization'''\n'''not needed from below'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count outliers\nimport numpy as np\n\nlower_bound_array = np.array([lower_bound[col] for col in numerical_columns])\nupper_bound_array = np.array([upper_bound[col] for col in numerical_columns])\n\noutliers = (train_data[numerical_columns]< lower_bound_array) | (train_data[numerical_columns] > upper_bound_array)\noutlier_counts = outliers.sum().compute()\nprint(\"Outlier counts:\\n\", outlier_counts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize to ensure clipping worked\nsampled_data = train_data.sample(frac=0.01).compute()\nsampled_data.boxplot(figsize=(20, 6))\nplt.xticks(rotation=90)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute summary statistics\nsummary = train_data[numerical_columns].describe(percentiles = [0.25, 0.5, 0.75]).compute()\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualize the data\n\nimport matplotlib.pyplot as plt\n\n# Sample some data to visualize\nsampled_data = train_data[numerical_columns].sample(frac=0.1).compute()\n\n# Plot boxplots\nsampled_data.boxplot(figsize=(12, 6))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute summary statistics\nsummary = train_data[numerical_columns].describe().compute()\n\n# Plot summary statistics\nsummary.T[['min', '25%', '50%', '75%', 'max']].plot(kind='bar', figsize=(12, 6))\nplt.title(\"Summary Statistics\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''After applying the interpolation, I am still left with many NaN values. To tackle this,\nI have two main courses of action, I can either completely drop these rows, or I can use median \nvalues as default placeholders. Although I believe that simply dropping these rows would be\nthe best strategy(given that they represent a very small portion of the data) There is no way \nto be sure.\n\nbecause of this, i have decided to maintain two version of the data, one based on dropping nan \nrows, and the other based on median filling. I can cross validate these two later to deterine\n                which is the best option to proceed with'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''The code beyond this point is soley for the purpose of documenting my data exploration/experimentatin. \nalthough it is mostly the same logic as above, it cannot be guarenteed to run, nor can \nit be guarenteed to provide clean data.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''This is the version that fills remaining NaNs with medians'''\n\nimport dask.dataframe as dd\n\n# Load the entire dataset with Dask\ntrain_data = dd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet')\n\n# Filter out the first 85 days\ntrain_data = train_data[train_data['date_id'] >= 85]\n\n# Drop fully empty (all-NaN) partitions to save memory\ndef drop_empty_partitions(df):\n    return df.dropna(how='all')\n\ntrain_data = train_data.map_partitions(drop_empty_partitions)\n\n# Define a function to interpolate within each partition\ndef interpolate_partition(df):\n    # Interpolate numerical columns only\n    numerical_columns = df.select_dtypes(include=['number']).columns\n    # Perform interpolation in multiple steps\n    df[numerical_columns] = df[numerical_columns].interpolate(method='linear', limit_direction='both')\n    return df\n\n# Apply the interpolation function to each partition\ntrain_data = train_data.map_partitions(interpolate_partition)\n\n# Compute medians for numerical columns\n# Sample a fraction of data to compute medians, adjusting `frac` as per memory constraints\nsample_data = train_data.sample(frac=0.1).compute()  # Adjust `frac` as necessary\n# Compute medians for numeric columns only\nnumeric_columns = sample_data.select_dtypes(include=['number'])\nmedians = numeric_columns.median()\n\n# Fill remaining NaNs in the dataset with median values for numeric columns\ntrain_data = train_data.map_partitions(\n    lambda df: df.fillna(medians.to_dict())\n)\n\n# Optional: Verify if there are any remaining NaNs\nremaining_na = train_data.isna().sum().sum().compute()\nprint(\"Remaining NaNs after interpolation and filling with medians:\", remaining_na)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''cannot use this unfortunately bc pandas df is too greedy\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n#Initialize scaler\n\nscaler = MinMaxScaler()\n\n#apply scaler to numerical columns\n\ntrain_data[numerical_columns] = train_data[numerical_columns].map_partitions(\n    lambda df: pd.DataFrame\n)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' this is no longer needed given that we are now using interprolation instead.\n\n#Apply fillna with a limit to handle short gaps but retain larger missing sections\ntrain_data = train_data.bfill(limit=5000).ffill(limit=5000)\n\n#Sample 10% of the data to compute medians\nsample_data = train_data.sample(frac=0.1).compute()\n\n#Select only numeric columns\nnumeric_columns = sample_data.select_dtypes(include=['number'])\nmedians = numeric_columns.median()\n\n# Step 4: Fill remaining NaNs using column-specific medians\ntrain_data = train_data.fillna(medians.to_dict())\n\n# Optional: Check for remaining NaNs if needed\nremaining_na = train_data.isna().sum().sum().compute()\nprint(\"Remaining NaNs after fill:\", remaining_na)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dask.dataframe as dd\n\n# Load the dataset in Dask (memory-efficient)\ndata_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\ntrain_data = dd.read_parquet(data_path)\n\n# Take a 10% random sample of the data for initial inspection\nsample_data = train_data.sample(frac=0.1, random_state=42).compute()  # Converts to pandas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Check the first few rows of the sample\nprint(sample_data.head())\n\n# Check the data types of each column\n#print(sample_data.dtypes)\n\n# Get general information about the data, including non-null counts\n#print(sample_data.info())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the total number of rows (compute once for efficiency)\ntotal_rows = len(train_data)\n\n# Count missing values in each column\nmissing_values = train_data.isna().sum().compute()  # .compute() to get concrete values in pandas\n\n# Identify columns where the count of NaNs is equal to the total number of rows\nall_nan_columns = missing_values[missing_values == total_rows].index\nprint(\"Columns with all NaN values:\", all_nan_columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test what percent of each column is filled with NaN values\n#will hopefully help determine which ones are causing the fill NaN error\n\nimport pandas as pd\n\n# Calculate NaN percentages\nnan_percentage = train_data.isna().mean().compute() * 100  # Compute to get concrete values in pandas\n\n# Sort columns by NaN percentage in descending order (highest NaN percentage at the top)\nnan_percentage_sorted = nan_percentage.sort_values(ascending=False)\n\n# Display all columns with NaN percentages\npd.set_option('display.max_rows', None)  # Show all rows without truncation\nprint(\"NaN percentage per column (sorted):\\n\", nan_percentage_sorted)\npd.reset_option('display.max_rows')  # Reset display option back to default\n\n'''\nNaN percentage per column (sorted):\n feature_26      17.900406\nfeature_21      17.900406\nfeature_27      17.900406\nfeature_31      17.900406\nfeature_42       9.125593\nfeature_39       9.125593\nfeature_50       9.026816\nfeature_53       9.026816\nfeature_00       6.752030\nfeature_01       6.752030\nfeature_02       6.752030\nfeature_03       6.752030\nfeature_04       6.752030\nfeature_15       2.566024\nfeature_41       2.319274\nfeature_44       2.319274\nfeature_52       2.217180\nfeature_55       2.217180\nfeature_74       1.026493\nfeature_73       1.026493\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Count missing values in each column\nmissing_values = sample_data.isna().sum()\nprint(\"Missing values per column:\\n\", missing_values)\n\n# Show only columns with missing values for better clarity\nmissing_columns = missing_values[missing_values > 0]\nprint(\"Columns with missing values:\\n\", missing_columns)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter to exclude the first 85 days\nsample_data = sample_data[sample_data['date_id'] >= 85]\nprint(\"Data after filtering the first 85 days:\", sample_data.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply backward-fill, then forward-fill\nsample_data = sample_data.bfill().ffill()\n\n# Verify that there are no remaining missing values\nprint(\"Remaining missing values:\", sample_data.isna().sum().sum())  # Should be 0 if all NaNs are filled\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}