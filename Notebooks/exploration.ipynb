{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''This is the version that drops the rows'''\n\nimport dask.dataframe as dd\n\n#Load the entire dataset with Dask\ntrain_data = dd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet')\n\n#Filter out the first 85 days\ntrain_data = train_data[train_data['date_id'] >= 85]\n#Drop fully empty (all-NaN) partitions to save memory\ndef drop_empty_partitions(df):\n    return df.dropna(how='all')\n\ntrain_data = train_data.map_partitions(drop_empty_partitions)\n\n#Define a function to interpolate within each partition(moved outside the function for later data observation)\nnumerical_columns = train_data.select_dtypes(include=['number']).columns\n\n# Updated interpolate function\ndef interpolate_partition(df):\n    # Perform interpolation\n    df[numerical_columns] = df[numerical_columns].interpolate(method='linear', limit_direction='both')\n    return df\n\n# Apply the interpolation function to each partition\ntrain_data = train_data.map_partitions(interpolate_partition)\ntrain_data = train_data.dropna(how='any')\n\n#Verify if there are any remaining NaNs\nremaining_na = train_data.isna().sum().sum().compute()\nprint(\"Remaining NaNs after interpolation:\", remaining_na)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:48:43.402850Z","iopub.execute_input":"2024-11-19T03:48:43.403677Z","iopub.status.idle":"2024-11-19T03:50:54.573342Z","shell.execute_reply.started":"2024-11-19T03:48:43.403630Z","shell.execute_reply":"2024-11-19T03:50:54.572127Z"}},"outputs":[{"name":"stdout","text":"Remaining NaNs after interpolation: 0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Compute summary statistics\nsummary = train_data[numerical_columns].describe(percentiles = [0.25, 0.5, 0.75]).compute()\nprint(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:52:13.685456Z","iopub.execute_input":"2024-11-19T03:52:13.685918Z","iopub.status.idle":"2024-11-19T03:55:08.962984Z","shell.execute_reply.started":"2024-11-19T03:52:13.685881Z","shell.execute_reply":"2024-11-19T03:55:08.961496Z"}},"outputs":[{"name":"stdout","text":"            date_id       time_id     symbol_id        weight    feature_00  \\\ncount  3.934201e+07  3.934201e+07  3.934201e+07  3.934201e+07  3.934201e+07   \nmean   1.148453e+03  4.775525e+02  1.879362e+01  2.044000e+00  5.943514e-01   \nstd    3.312345e+02  2.767718e+02  1.122486e+01  1.143265e+00  1.346729e+00   \nmin    5.100000e+02  0.000000e+00  0.000000e+00  1.499667e-01 -5.794129e+00   \n25%    8.710000e+02  2.580000e+02  1.100000e+01  1.476307e+00 -3.038115e-02   \n50%    1.168000e+03  5.090000e+02  2.000000e+01  2.106626e+00  8.334433e-01   \n75%    1.434000e+03  7.480000e+02  2.900000e+01  4.103159e+00  2.206827e+00   \nmax    1.698000e+03  9.670000e+02  3.800000e+01  1.024042e+01  6.477002e+00   \n\n         feature_01    feature_02    feature_03    feature_04    feature_05  \\\ncount  3.934201e+07  3.934201e+07  3.934201e+07  3.934201e+07  3.934201e+07   \nmean   1.045648e-02  5.931975e-01  5.929216e-01  6.066390e-04 -4.811674e-02   \nstd    1.101107e+00  1.341614e+00  1.341667e+00  1.047886e+00  1.017604e+00   \nmin   -4.953487e+00 -5.726010e+00 -5.601890e+00 -5.799880e+00 -2.535040e+01   \n25%   -3.460638e-01 -1.580641e-02 -1.959961e-02 -4.667797e-01 -3.308480e-01   \n50%    3.486357e-01  8.231779e-01  8.225300e-01  1.134397e-01  1.158459e-01   \n75%    1.405966e+00  2.227911e+00  2.196533e+00  1.145805e+00  1.027239e+00   \nmax    6.292391e+00  6.490265e+00  6.695623e+00  6.164160e+00  3.572622e+01   \n\n       ...    feature_78   responder_0   responder_1   responder_2  \\\ncount  ...  3.934201e+07  3.934201e+07  3.934201e+07  3.934201e+07   \nmean   ... -1.037759e-02 -2.761282e-03 -2.973503e-03 -4.177576e-04   \nstd    ...  9.606067e-01  5.274490e-01  4.947928e-01  5.422781e-01   \nmin    ... -6.159515e+00 -5.000000e+00 -5.000000e+00 -5.000000e+00   \n25%    ... -2.447366e-01 -1.319363e-01 -1.077630e-01 -1.272496e-01   \n50%    ... -1.029286e-01  4.871721e-03  1.293741e-02  1.847853e-03   \n75%    ...  3.884036e-01  4.061097e-01  3.367091e-01  4.102618e-01   \nmax    ...  2.574540e+02  5.000000e+00  5.000000e+00  5.000000e+00   \n\n        responder_3   responder_4   responder_5   responder_6   responder_7  \\\ncount  3.934201e+07  3.934201e+07  3.934201e+07  3.934201e+07  3.934201e+07   \nmean  -1.941543e-02 -1.488989e-02 -1.949754e-02 -1.517173e-03  3.773163e-03   \nstd    7.822643e-01  8.288495e-01  6.964017e-01  8.860156e-01  9.100691e-01   \nmin   -5.000000e+00 -5.000000e+00 -5.000000e+00 -5.000000e+00 -5.000000e+00   \n25%   -2.097922e-01 -2.580736e-01 -1.559837e-01 -2.657704e-01 -2.742171e-01   \n50%    1.666263e-02  1.674504e-02  1.926651e-02 -3.626317e-03 -4.533414e-03   \n75%    4.168953e-01  4.779227e-01  3.394354e-01  5.354323e-01  5.595564e-01   \nmax    5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00   \n\n        responder_8  \ncount  3.934201e+07  \nmean  -8.624149e-04  \nstd    8.563494e-01  \nmin   -5.000000e+00  \n25%   -2.373489e-01  \n50%    2.794336e-03  \n75%    5.224141e-01  \nmax    5.000000e+00  \n\n[8 rows x 92 columns]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Compute IQR thresholds\nq1 = train_data[numerical_columns].quantile(0.25).compute()\nq3 = train_data[numerical_columns].quantile(0.75).compute()\niqr = q3 - q1\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\nprint(\"Lower bounds:\\n\", lower_bound)\nprint(\"Upper bounds:\\n\", upper_bound)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:55:08.965722Z","iopub.execute_input":"2024-11-19T03:55:08.966322Z","iopub.status.idle":"2024-11-19T03:59:02.964027Z","shell.execute_reply.started":"2024-11-19T03:55:08.966269Z","shell.execute_reply":"2024-11-19T03:59:02.962410Z"}},"outputs":[{"name":"stdout","text":"Lower bounds:\n date_id         26.500000\ntime_id       -477.000000\nsymbol_id      -16.000000\nweight          -0.930002\nfeature_00      -0.802240\n                  ...    \nresponder_4     -1.362068\nresponder_5     -0.899112\nresponder_6     -1.467574\nresponder_7     -1.524877\nresponder_8     -1.376993\nLength: 92, dtype: float64\nUpper bounds:\n date_id        2278.500000\ntime_id        1483.000000\nsymbol_id        56.000000\nweight            7.194063\nfeature_00        4.845968\n                  ...     \nresponder_4       1.581917\nresponder_5       1.082564\nresponder_6       1.737236\nresponder_7       1.810217\nresponder_8       1.662059\nLength: 92, dtype: float64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Sample some data to visualize\nsampled_data = train_data[numerical_columns].sample(frac=0.1).compute()\n\n# Plot boxplots\nsampled_data.boxplot(figsize=(12, 6))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''After applying the interpolation, I am still left with many NaN values. To tackle this,\nI have two main courses of action, I can either completely drop these rows, or I can use median \nvalues as default placeholders. Although I believe that simply dropping these rows would be\nthe best strategy(given that they represent a very small portion of the data) There is no way \nto be sure.\n\nbecause of this, i have decided to maintain two version of the data, one based on dropping nan \nrows, and the other based on median filling. I can cross validate these two later to deterine\n                which is the best option to proceed with'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''The code beyond this point is soley for the purpose of documenting my data exploration/experimentatin. \nalthough it is mostly the same logic as above, it cannot be guarenteed to run, nor can \nit be guarenteed to provide clean data.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''This is the version that fills remaining NaNs with medians'''\n\nimport dask.dataframe as dd\n\n# Load the entire dataset with Dask\ntrain_data = dd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet')\n\n# Filter out the first 85 days\ntrain_data = train_data[train_data['date_id'] >= 85]\n\n# Drop fully empty (all-NaN) partitions to save memory\ndef drop_empty_partitions(df):\n    return df.dropna(how='all')\n\ntrain_data = train_data.map_partitions(drop_empty_partitions)\n\n# Define a function to interpolate within each partition\ndef interpolate_partition(df):\n    # Interpolate numerical columns only\n    numerical_columns = df.select_dtypes(include=['number']).columns\n    # Perform interpolation in multiple steps\n    df[numerical_columns] = df[numerical_columns].interpolate(method='linear', limit_direction='both')\n    return df\n\n# Apply the interpolation function to each partition\ntrain_data = train_data.map_partitions(interpolate_partition)\n\n# Compute medians for numerical columns\n# Sample a fraction of data to compute medians, adjusting `frac` as per memory constraints\nsample_data = train_data.sample(frac=0.1).compute()  # Adjust `frac` as necessary\n# Compute medians for numeric columns only\nnumeric_columns = sample_data.select_dtypes(include=['number'])\nmedians = numeric_columns.median()\n\n# Fill remaining NaNs in the dataset with median values for numeric columns\ntrain_data = train_data.map_partitions(\n    lambda df: df.fillna(medians.to_dict())\n)\n\n# Optional: Verify if there are any remaining NaNs\nremaining_na = train_data.isna().sum().sum().compute()\nprint(\"Remaining NaNs after interpolation and filling with medians:\", remaining_na)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' this is no longer needed given that we are now using interprolation instead.\n\n#Apply fillna with a limit to handle short gaps but retain larger missing sections\ntrain_data = train_data.bfill(limit=5000).ffill(limit=5000)\n\n#Sample 10% of the data to compute medians\nsample_data = train_data.sample(frac=0.1).compute()\n\n#Select only numeric columns\nnumeric_columns = sample_data.select_dtypes(include=['number'])\nmedians = numeric_columns.median()\n\n# Step 4: Fill remaining NaNs using column-specific medians\ntrain_data = train_data.fillna(medians.to_dict())\n\n# Optional: Check for remaining NaNs if needed\nremaining_na = train_data.isna().sum().sum().compute()\nprint(\"Remaining NaNs after fill:\", remaining_na)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dask.dataframe as dd\n\n# Load the dataset in Dask (memory-efficient)\ndata_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\ntrain_data = dd.read_parquet(data_path)\n\n# Take a 10% random sample of the data for initial inspection\nsample_data = train_data.sample(frac=0.1, random_state=42).compute()  # Converts to pandas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Check the first few rows of the sample\nprint(sample_data.head())\n\n# Check the data types of each column\n#print(sample_data.dtypes)\n\n# Get general information about the data, including non-null counts\n#print(sample_data.info())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the total number of rows (compute once for efficiency)\ntotal_rows = len(train_data)\n\n# Count missing values in each column\nmissing_values = train_data.isna().sum().compute()  # .compute() to get concrete values in pandas\n\n# Identify columns where the count of NaNs is equal to the total number of rows\nall_nan_columns = missing_values[missing_values == total_rows].index\nprint(\"Columns with all NaN values:\", all_nan_columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test what percent of each column is filled with NaN values\n#will hopefully help determine which ones are causing the fill NaN error\n\nimport pandas as pd\n\n# Calculate NaN percentages\nnan_percentage = train_data.isna().mean().compute() * 100  # Compute to get concrete values in pandas\n\n# Sort columns by NaN percentage in descending order (highest NaN percentage at the top)\nnan_percentage_sorted = nan_percentage.sort_values(ascending=False)\n\n# Display all columns with NaN percentages\npd.set_option('display.max_rows', None)  # Show all rows without truncation\nprint(\"NaN percentage per column (sorted):\\n\", nan_percentage_sorted)\npd.reset_option('display.max_rows')  # Reset display option back to default\n\n'''\nNaN percentage per column (sorted):\n feature_26      17.900406\nfeature_21      17.900406\nfeature_27      17.900406\nfeature_31      17.900406\nfeature_42       9.125593\nfeature_39       9.125593\nfeature_50       9.026816\nfeature_53       9.026816\nfeature_00       6.752030\nfeature_01       6.752030\nfeature_02       6.752030\nfeature_03       6.752030\nfeature_04       6.752030\nfeature_15       2.566024\nfeature_41       2.319274\nfeature_44       2.319274\nfeature_52       2.217180\nfeature_55       2.217180\nfeature_74       1.026493\nfeature_73       1.026493\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Count missing values in each column\nmissing_values = sample_data.isna().sum()\nprint(\"Missing values per column:\\n\", missing_values)\n\n# Show only columns with missing values for better clarity\nmissing_columns = missing_values[missing_values > 0]\nprint(\"Columns with missing values:\\n\", missing_columns)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter to exclude the first 85 days\nsample_data = sample_data[sample_data['date_id'] >= 85]\nprint(\"Data after filtering the first 85 days:\", sample_data.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply backward-fill, then forward-fill\nsample_data = sample_data.bfill().ffill()\n\n# Verify that there are no remaining missing values\nprint(\"Remaining missing values:\", sample_data.isna().sum().sum())  # Should be 0 if all NaNs are filled\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}