{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7200fc-bd3b-47ef-a465-29996ffc954d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "df = pl.read_parquet(\"/home/jupyter/data/int32Features.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c683a-ad33-4aaf-bf83-97c4b5d4492c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for null values in each column\n",
    "null_counts = df.null_count()\n",
    "\n",
    "# Display columns with null values and their counts\n",
    "for column, count in zip(null_counts.columns, null_counts.row(0)):\n",
    "    print(f\"{column}: {count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf7acd-61f9-4ee6-9572-c91e1ccbc0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score\n",
    "import logging\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Reproducibility Setup\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False  # Safer for reproducibility\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Constants\n",
    "HIDDEN_DIM = 512\n",
    "DROPOUT_RATE = 0.2\n",
    "NOISE_STD = 0.001\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 256\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 75\n",
    "N_FOLDS = 5\n",
    "GAP_SIZE = 30\n",
    "VAL_SIZE = 100\n",
    "TRAIN_SIZE = 300\n",
    "GRADIENT_CLIP_VALUE = 0.5\n",
    "LR_PATIENCE = 5\n",
    "MIN_LR = 1e-7\n",
    "\n",
    "# Paths\n",
    "PARTITIONED_DIR = \"/home/jupyter/data/partitioned/\"\n",
    "all_part_files = sorted(glob.glob(os.path.join(PARTITIONED_DIR, \"*.parquet\")))\n",
    "logger.info(f\"Found {len(all_part_files)} partitioned files.\")\n",
    "\n",
    "# Exclude columns\n",
    "EXCLUDE_COLS = [\"date_id\", \"time_id\", \"symbol_id\", \"weight\", \"partition_id\"]\n",
    "df_tmp = pl.read_parquet(all_part_files[0])\n",
    "EXCLUDE_COLS += [col for col in df_tmp.columns if col.startswith(\"responder_\")]\n",
    "del df_tmp\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, features, target, weights):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx], self.weights[idx]\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.05):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.std > 0:\n",
    "            return x + torch.randn_like(x) * self.std\n",
    "        return x\n",
    "\n",
    "class AutoencoderWithMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, dropout_rate=0.3, noise_std=0.05):\n",
    "        super().__init__()\n",
    "        self.noise = GaussianNoise(std=noise_std)\n",
    "        \n",
    "        # Smaller initial hidden dimensions with gradual scaling\n",
    "        dim1 = hidden_dim // 2  # 256\n",
    "        dim2 = hidden_dim // 4  # 128\n",
    "        dim3 = hidden_dim // 8  # 64\n",
    "        \n",
    "        # Layer normalization for better stability\n",
    "        self.input_norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim1),\n",
    "            nn.LayerNorm(dim1),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(dim1, dim2),\n",
    "            nn.LayerNorm(dim2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(dim2, dim3),\n",
    "            nn.LayerNorm(dim3)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim3, dim2),\n",
    "            nn.LayerNorm(dim2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(dim2, input_dim),\n",
    "            nn.LayerNorm(input_dim)\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim + dim3, dim1),\n",
    "            nn.LayerNorm(dim1),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(dim1, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.1, mode='fan_in', nonlinearity='linear')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_norm(x)\n",
    "        x_noised = self.noise(x)\n",
    "        \n",
    "        encoded = self.encoder(x_noised)\n",
    "        latent = self.bottleneck(encoded)\n",
    "        \n",
    "        reconstructed = self.decoder(latent)\n",
    "        reconstructed = reconstructed + x  # Skip connection\n",
    "        \n",
    "        mlp_input = torch.cat([x, latent], dim=1)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        \n",
    "        return reconstructed, mlp_output, latent\n",
    "\n",
    "import math\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"Initialize weights using Kaiming He initialization with manual gain for SiLU.\"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # Manual gain for SiLU (approximated)\n",
    "            gain = math.sqrt(2)\n",
    "            nn.init.kaiming_normal_(m.weight, a=gain, nonlinearity=\"leaky_relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def get_unique_dates(files):\n",
    "    all_dates = set()\n",
    "    for file in files:\n",
    "        df = pl.read_parquet(file).select(\"date_id\")\n",
    "        unique_dates = df.unique().to_series().to_list()\n",
    "        all_dates.update(unique_dates)\n",
    "    return sorted(all_dates)\n",
    "\n",
    "def load_and_scale_data_for_dates(\n",
    "    parquet_file, date_list, feature_scaler, exclude_cols, \n",
    "    target_col=\"responder_6\", weight_col=\"weight\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a Parquet file, filter rows to only those date_ids in date_list,\n",
    "    apply scaling, and return (features, targets, weights).\n",
    "    If no rows match, or all are dropped as null, returns (None, None, None).\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(parquet_file)\n",
    "    df = df.filter(pl.col(\"date_id\").is_in(date_list))\n",
    "    if df.is_empty():\n",
    "        return None, None, None\n",
    "\n",
    "    numeric_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    needed_cols = numeric_cols + [target_col, weight_col]\n",
    "\n",
    "    # Drop any rows containing null in these columns\n",
    "    df = df.drop_nulls(subset=needed_cols)\n",
    "    if df.is_empty():\n",
    "        return None, None, None\n",
    "\n",
    "    features = df.select(numeric_cols).to_numpy().astype(np.float32)\n",
    "    targets = df[target_col].to_numpy().astype(np.float32)\n",
    "    weights = df[weight_col].to_numpy().astype(np.float32)\n",
    "\n",
    "    # Scale features and ensure no NaNs remain\n",
    "    features = feature_scaler.transform(features)\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    if np.any(np.abs(features) > 100):\n",
    "        features = np.clip(features, -100, 100)\n",
    "    # Normalize targets (standardization)\n",
    "    targets_mean = targets.mean()\n",
    "    targets_std = targets.std() + 1e-6  # Avoid division by zero\n",
    "    targets = (targets - targets_mean) / targets_std  # <- Added normalization\n",
    "    \n",
    "    return features, targets, weights\n",
    "\n",
    "def create_rolling_folds(dates_list, n_folds, gap_size, val_size, train_size):\n",
    "    folds = []\n",
    "    idx_start = 0\n",
    "    total_dates = len(dates_list)\n",
    "\n",
    "    for _ in range(n_folds):\n",
    "        train_end = idx_start + train_size\n",
    "        gap_end = train_end + gap_size\n",
    "        val_end = gap_end + val_size\n",
    "\n",
    "        if val_end > total_dates:\n",
    "            break\n",
    "\n",
    "        train_dates = dates_list[idx_start:train_end]\n",
    "        val_dates = dates_list[gap_end:val_end]\n",
    "        \n",
    "        folds.append((train_dates, val_dates))\n",
    "        idx_start = val_end\n",
    "\n",
    "    return folds\n",
    "\n",
    "# Build or load the scaler\n",
    "if os.path.exists(\"feature_scaler.pkl\"):\n",
    "    with open(\"feature_scaler.pkl\", \"rb\") as f:\n",
    "        feature_scaler = pickle.load(f)\n",
    "else:\n",
    "    df_tmp = pl.read_parquet(all_part_files[0])\n",
    "    numeric_cols = [col for col in df_tmp.columns if col not in EXCLUDE_COLS]\n",
    "    features = df_tmp.select(numeric_cols).to_numpy()\n",
    "    feature_scaler = StandardScaler().fit(features)\n",
    "    with open(\"feature_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(feature_scaler, f)\n",
    "    del df_tmp\n",
    "\n",
    "GLOBAL_INPUT_DIM = feature_scaler.n_features_in_\n",
    "sorted_dates = get_unique_dates(all_part_files)\n",
    "folds = create_rolling_folds(sorted_dates, N_FOLDS, GAP_SIZE, VAL_SIZE, TRAIN_SIZE)\n",
    "\n",
    "logger.info(f\"Created {len(folds)} folds\")\n",
    "\n",
    "def train_model_with_cv(folds, files, feature_scaler):\n",
    "    for fold_idx, (train_dates, val_dates) in enumerate(folds, start=1):\n",
    "        logger.info(f\"Training Fold {fold_idx}/{len(folds)}...\")\n",
    "\n",
    "        # Model and optimizer setup\n",
    "        model = AutoencoderWithMLP(\n",
    "            input_dim=GLOBAL_INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            dropout_rate=DROPOUT_RATE,\n",
    "            noise_std=NOISE_STD\n",
    "        )\n",
    "        initialize_weights(model)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = LEARNING_RATE\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=LEARNING_RATE,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=len(train_loader) if 'train_loader' in locals() else 100,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        recon_criterion = nn.MSELoss()\n",
    "        supervised_criterion = nn.MSELoss()\n",
    "        scaler = torch.amp.GradScaler()\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=7)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            # ----------------------------\n",
    "            # Training Phase\n",
    "            # ----------------------------\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_steps = 0\n",
    "\n",
    "            for file in files:\n",
    "                features, targets, weights = load_and_scale_data_for_dates(\n",
    "                    file, train_dates, feature_scaler, EXCLUDE_COLS\n",
    "                )\n",
    "                if features is None:\n",
    "                    continue\n",
    "\n",
    "                train_dataset = FinancialDataset(features, targets, weights)\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                    num_workers=4, pin_memory=True\n",
    "                )\n",
    "\n",
    "                for batch_x, batch_y, batch_w in train_loader:\n",
    "                    batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                        reconstructed, mlp_output, _ = model(batch_x)\n",
    "                        loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                        loss_sup = supervised_criterion(mlp_output.squeeze(), batch_y)\n",
    "                        # Safer weighted loss to avoid NaN issues\n",
    "                        loss_sup = (loss_sup * batch_w).sum() / (batch_w.sum() + 1e-6)\n",
    "                        loss = loss_recon + loss_sup\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    train_steps += 1\n",
    "\n",
    "            avg_train_loss = train_loss / train_steps if train_steps > 0 else float('nan')\n",
    "\n",
    "            # ----------------------------\n",
    "            # Validation Phase\n",
    "            # ----------------------------\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_steps = 0\n",
    "            all_targets, all_predictions = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for file in files:\n",
    "                    features, targets, weights = load_and_scale_data_for_dates(\n",
    "                        file, val_dates, feature_scaler, EXCLUDE_COLS\n",
    "                    )\n",
    "                    if features is None:\n",
    "                        continue\n",
    "\n",
    "                    val_dataset = FinancialDataset(features, targets, weights)\n",
    "                    val_loader = DataLoader(\n",
    "                        val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=4, pin_memory=True\n",
    "                    )\n",
    "\n",
    "                    for batch_x, batch_y, batch_w in val_loader:\n",
    "                        batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "\n",
    "                        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "\n",
    "                            reconstructed, mlp_output, _ = model(batch_x)\n",
    "                            loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                            loss_sup = supervised_criterion(mlp_output.squeeze(), batch_y)\n",
    "                            loss_sup = (loss_sup * batch_w).sum() / (batch_w.sum() + 1e-6)\n",
    "                            loss = loss_recon + loss_sup\n",
    "\n",
    "                        val_loss += loss.item()\n",
    "                        val_steps += 1\n",
    "\n",
    "                        # Collect predictions and targets for R² calculation\n",
    "                        all_targets.extend(batch_y.cpu().numpy())\n",
    "                        all_predictions.extend(mlp_output.squeeze().cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / val_steps if val_steps > 0 else float('nan')\n",
    "\n",
    "            # R² calculation with NaN protection\n",
    "            all_targets = np.array(all_targets)\n",
    "            all_predictions = np.array(all_predictions)\n",
    "            valid_mask = np.isfinite(all_targets) & np.isfinite(all_predictions)\n",
    "            filtered_targets = all_targets[valid_mask]\n",
    "            filtered_predictions = all_predictions[valid_mask]\n",
    "            r2 = r2_score(filtered_targets, filtered_predictions) if len(filtered_targets) > 1 else float('nan')\n",
    "\n",
    "            logger.info(f\"[Fold {fold_idx} | Epoch {epoch}] \"\n",
    "                        f\"Train Loss: {avg_train_loss:.5f} | \"\n",
    "                        f\"Val Loss: {avg_val_loss:.5f} | \"\n",
    "                        f\"R²: {r2:.5f}\")\n",
    "\n",
    "            # ----------------------------\n",
    "            # Save the best model\n",
    "            # ----------------------------\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), f\"best_model_fold_{fold_idx}.pt\")\n",
    "                logger.info(f\"Best model updated for fold {fold_idx}\")\n",
    "\n",
    "            # Step scheduler and early stopping\n",
    "            scheduler.step()\n",
    "            early_stopping(avg_val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(f\"Early stopping triggered for Fold {fold_idx}\")\n",
    "                break\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Main Execution (unchanged from original)\n",
    "# ----------------------------------------------------------------------------\n",
    "sorted_dates = get_unique_dates(all_part_files)\n",
    "folds = create_rolling_folds(sorted_dates, N_FOLDS, GAP_SIZE, VAL_SIZE, TRAIN_SIZE)\n",
    "logger.info(f\"Created {len(folds)} folds\")\n",
    "\n",
    "train_model_with_cv(folds, all_part_files, feature_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54337e85-3258-4912-910c-74b65a9d159a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 11:56:51,871 [INFO] Using device: cuda\n",
      "2025-01-13 11:56:51,874 [INFO] Found 9 partitioned files.\n",
      "2025-01-13 11:57:08,631 [INFO] Created 3 folds\n",
      "2025-01-13 11:57:22,091 [INFO] Created 3 folds\n",
      "2025-01-13 11:57:22,092 [INFO] Training Fold 1/3...\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "If you really know what your doing, you can silence this warning with the warning module\n",
      "or by setting POLARS_ALLOW_FORKING_THREAD=1.\n",
      "\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "If you really know what your doing, you can silence this warning with the warning module\n",
      "or by setting POLARS_ALLOW_FORKING_THREAD=1.\n",
      "\n",
      "  self.pid = os.fork()\n",
      "2025-01-13 12:01:14,160 [INFO] [Fold 1 | Epoch 1] Train Loss: 2.27464 | Val Loss: 2.10314 | R²: -0.03081\n",
      "2025-01-13 12:01:14,168 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:05:10,262 [INFO] [Fold 1 | Epoch 2] Train Loss: 2.12890 | Val Loss: 2.02629 | R²: -0.01413\n",
      "2025-01-13 12:05:10,269 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:09:05,250 [INFO] [Fold 1 | Epoch 3] Train Loss: 2.04511 | Val Loss: 1.97024 | R²: -0.00742\n",
      "2025-01-13 12:09:05,259 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:13:03,278 [INFO] [Fold 1 | Epoch 4] Train Loss: 1.97863 | Val Loss: 1.92088 | R²: -0.00345\n",
      "2025-01-13 12:13:03,287 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:16:57,940 [INFO] [Fold 1 | Epoch 5] Train Loss: 1.92109 | Val Loss: 1.87548 | R²: -0.00099\n",
      "2025-01-13 12:16:57,948 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:20:52,396 [INFO] [Fold 1 | Epoch 6] Train Loss: 1.87017 | Val Loss: 1.83303 | R²: 0.00077\n",
      "2025-01-13 12:20:52,404 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:24:47,533 [INFO] [Fold 1 | Epoch 7] Train Loss: 1.82365 | Val Loss: 1.79336 | R²: 0.00194\n",
      "2025-01-13 12:24:47,539 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:28:39,595 [INFO] [Fold 1 | Epoch 8] Train Loss: 1.78090 | Val Loss: 1.75547 | R²: 0.00313\n",
      "2025-01-13 12:28:39,603 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:32:28,774 [INFO] [Fold 1 | Epoch 9] Train Loss: 1.74126 | Val Loss: 1.71954 | R²: 0.00376\n",
      "2025-01-13 12:32:28,782 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:36:19,218 [INFO] [Fold 1 | Epoch 10] Train Loss: 1.70442 | Val Loss: 1.68461 | R²: 0.00458\n",
      "2025-01-13 12:36:19,226 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:40:08,075 [INFO] [Fold 1 | Epoch 11] Train Loss: 1.66915 | Val Loss: 1.65119 | R²: 0.00502\n",
      "2025-01-13 12:40:08,082 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:43:57,176 [INFO] [Fold 1 | Epoch 12] Train Loss: 1.63554 | Val Loss: 1.61888 | R²: 0.00541\n",
      "2025-01-13 12:43:57,183 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:47:48,256 [INFO] [Fold 1 | Epoch 13] Train Loss: 1.60356 | Val Loss: 1.58753 | R²: 0.00580\n",
      "2025-01-13 12:47:48,267 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:51:38,647 [INFO] [Fold 1 | Epoch 14] Train Loss: 1.57295 | Val Loss: 1.55730 | R²: 0.00610\n",
      "2025-01-13 12:51:38,655 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:55:37,727 [INFO] [Fold 1 | Epoch 15] Train Loss: 1.54336 | Val Loss: 1.52791 | R²: 0.00646\n",
      "2025-01-13 12:55:37,734 [INFO] Best model updated for fold 1\n",
      "2025-01-13 12:59:29,433 [INFO] [Fold 1 | Epoch 16] Train Loss: 1.51533 | Val Loss: 1.49970 | R²: 0.00672\n",
      "2025-01-13 12:59:29,440 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:03:26,097 [INFO] [Fold 1 | Epoch 17] Train Loss: 1.48820 | Val Loss: 1.47253 | R²: 0.00689\n",
      "2025-01-13 13:03:26,105 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:07:20,542 [INFO] [Fold 1 | Epoch 18] Train Loss: 1.46232 | Val Loss: 1.44638 | R²: 0.00707\n",
      "2025-01-13 13:07:20,549 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:11:11,825 [INFO] [Fold 1 | Epoch 19] Train Loss: 1.43746 | Val Loss: 1.42120 | R²: 0.00729\n",
      "2025-01-13 13:11:11,832 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:15:08,521 [INFO] [Fold 1 | Epoch 20] Train Loss: 1.41384 | Val Loss: 1.39711 | R²: 0.00749\n",
      "2025-01-13 13:15:08,529 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:19:03,954 [INFO] [Fold 1 | Epoch 21] Train Loss: 1.39109 | Val Loss: 1.37418 | R²: 0.00757\n",
      "2025-01-13 13:19:03,962 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:23:04,781 [INFO] [Fold 1 | Epoch 22] Train Loss: 1.36980 | Val Loss: 1.35216 | R²: 0.00778\n",
      "2025-01-13 13:23:04,788 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:27:09,241 [INFO] [Fold 1 | Epoch 23] Train Loss: 1.34918 | Val Loss: 1.33133 | R²: 0.00791\n",
      "2025-01-13 13:27:09,249 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:31:10,265 [INFO] [Fold 1 | Epoch 24] Train Loss: 1.32997 | Val Loss: 1.31166 | R²: 0.00793\n",
      "2025-01-13 13:31:10,272 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:35:09,473 [INFO] [Fold 1 | Epoch 25] Train Loss: 1.31142 | Val Loss: 1.29301 | R²: 0.00803\n",
      "2025-01-13 13:35:09,481 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:39:07,969 [INFO] [Fold 1 | Epoch 26] Train Loss: 1.29428 | Val Loss: 1.27539 | R²: 0.00818\n",
      "2025-01-13 13:39:07,977 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:43:04,355 [INFO] [Fold 1 | Epoch 27] Train Loss: 1.27810 | Val Loss: 1.25888 | R²: 0.00832\n",
      "2025-01-13 13:43:04,366 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:47:03,340 [INFO] [Fold 1 | Epoch 28] Train Loss: 1.26301 | Val Loss: 1.24359 | R²: 0.00836\n",
      "2025-01-13 13:47:03,347 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:51:02,763 [INFO] [Fold 1 | Epoch 29] Train Loss: 1.24881 | Val Loss: 1.22924 | R²: 0.00849\n",
      "2025-01-13 13:51:02,770 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:55:02,445 [INFO] [Fold 1 | Epoch 30] Train Loss: 1.23571 | Val Loss: 1.21602 | R²: 0.00851\n",
      "2025-01-13 13:55:02,453 [INFO] Best model updated for fold 1\n",
      "2025-01-13 13:59:01,122 [INFO] [Fold 1 | Epoch 31] Train Loss: 1.22344 | Val Loss: 1.20382 | R²: 0.00863\n",
      "2025-01-13 13:59:01,130 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:02:58,563 [INFO] [Fold 1 | Epoch 32] Train Loss: 1.21230 | Val Loss: 1.19259 | R²: 0.00876\n",
      "2025-01-13 14:02:58,570 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:06:55,866 [INFO] [Fold 1 | Epoch 33] Train Loss: 1.20212 | Val Loss: 1.18246 | R²: 0.00877\n",
      "2025-01-13 14:06:55,873 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:10:51,980 [INFO] [Fold 1 | Epoch 34] Train Loss: 1.19285 | Val Loss: 1.17325 | R²: 0.00885\n",
      "2025-01-13 14:10:51,987 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:14:47,029 [INFO] [Fold 1 | Epoch 35] Train Loss: 1.18445 | Val Loss: 1.16502 | R²: 0.00893\n",
      "2025-01-13 14:14:47,037 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:18:45,463 [INFO] [Fold 1 | Epoch 36] Train Loss: 1.17687 | Val Loss: 1.15782 | R²: 0.00896\n",
      "2025-01-13 14:18:45,471 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:22:41,870 [INFO] [Fold 1 | Epoch 37] Train Loss: 1.17022 | Val Loss: 1.15151 | R²: 0.00903\n",
      "2025-01-13 14:22:41,877 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:26:37,019 [INFO] [Fold 1 | Epoch 38] Train Loss: 1.16446 | Val Loss: 1.14617 | R²: 0.00906\n",
      "2025-01-13 14:26:37,027 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:30:32,786 [INFO] [Fold 1 | Epoch 39] Train Loss: 1.15960 | Val Loss: 1.14180 | R²: 0.00913\n",
      "2025-01-13 14:30:32,793 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:34:31,110 [INFO] [Fold 1 | Epoch 40] Train Loss: 1.15555 | Val Loss: 1.13838 | R²: 0.00918\n",
      "2025-01-13 14:34:31,117 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:38:29,876 [INFO] [Fold 1 | Epoch 41] Train Loss: 1.15196 | Val Loss: 1.13583 | R²: 0.00927\n",
      "2025-01-13 14:38:29,883 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:42:27,652 [INFO] [Fold 1 | Epoch 42] Train Loss: 1.14910 | Val Loss: 1.13424 | R²: 0.00926\n",
      "2025-01-13 14:42:27,659 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:46:24,381 [INFO] [Fold 1 | Epoch 43] Train Loss: 1.14687 | Val Loss: 1.13328 | R²: 0.00926\n",
      "2025-01-13 14:46:24,389 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:50:21,194 [INFO] [Fold 1 | Epoch 44] Train Loss: 1.14498 | Val Loss: 1.13272 | R²: 0.00933\n",
      "2025-01-13 14:50:21,201 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:54:18,336 [INFO] [Fold 1 | Epoch 45] Train Loss: 1.14338 | Val Loss: 1.13236 | R²: 0.00943\n",
      "2025-01-13 14:54:18,344 [INFO] Best model updated for fold 1\n",
      "2025-01-13 14:58:15,559 [INFO] [Fold 1 | Epoch 46] Train Loss: 1.14205 | Val Loss: 1.13213 | R²: 0.00948\n",
      "2025-01-13 14:58:15,566 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:02:13,036 [INFO] [Fold 1 | Epoch 47] Train Loss: 1.14066 | Val Loss: 1.13196 | R²: 0.00950\n",
      "2025-01-13 15:02:13,044 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:06:11,138 [INFO] [Fold 1 | Epoch 48] Train Loss: 1.13963 | Val Loss: 1.13174 | R²: 0.00953\n",
      "2025-01-13 15:06:11,145 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:10:08,916 [INFO] [Fold 1 | Epoch 49] Train Loss: 1.13861 | Val Loss: 1.13137 | R²: 0.00967\n",
      "2025-01-13 15:10:08,923 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:14:06,879 [INFO] [Fold 1 | Epoch 50] Train Loss: 1.13746 | Val Loss: 1.13120 | R²: 0.00958\n",
      "2025-01-13 15:14:06,887 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:18:05,720 [INFO] [Fold 1 | Epoch 51] Train Loss: 1.13649 | Val Loss: 1.13091 | R²: 0.00960\n",
      "2025-01-13 15:18:05,728 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:22:08,068 [INFO] [Fold 1 | Epoch 52] Train Loss: 1.13561 | Val Loss: 1.13056 | R²: 0.00969\n",
      "2025-01-13 15:22:08,079 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:26:07,610 [INFO] [Fold 1 | Epoch 53] Train Loss: 1.13452 | Val Loss: 1.13023 | R²: 0.00971\n",
      "2025-01-13 15:26:07,619 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:30:07,225 [INFO] [Fold 1 | Epoch 54] Train Loss: 1.13368 | Val Loss: 1.12994 | R²: 0.00971\n",
      "2025-01-13 15:30:07,233 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:34:06,111 [INFO] [Fold 1 | Epoch 55] Train Loss: 1.13300 | Val Loss: 1.12958 | R²: 0.00977\n",
      "2025-01-13 15:34:06,118 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:38:02,530 [INFO] [Fold 1 | Epoch 56] Train Loss: 1.13202 | Val Loss: 1.12931 | R²: 0.00978\n",
      "2025-01-13 15:38:02,537 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:42:00,298 [INFO] [Fold 1 | Epoch 57] Train Loss: 1.13122 | Val Loss: 1.12897 | R²: 0.00983\n",
      "2025-01-13 15:42:00,306 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:45:58,009 [INFO] [Fold 1 | Epoch 58] Train Loss: 1.13038 | Val Loss: 1.12867 | R²: 0.00985\n",
      "2025-01-13 15:45:58,017 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:49:59,539 [INFO] [Fold 1 | Epoch 59] Train Loss: 1.12958 | Val Loss: 1.12831 | R²: 0.00994\n",
      "2025-01-13 15:49:59,548 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:53:54,708 [INFO] [Fold 1 | Epoch 60] Train Loss: 1.12876 | Val Loss: 1.12805 | R²: 0.00991\n",
      "2025-01-13 15:53:54,715 [INFO] Best model updated for fold 1\n",
      "2025-01-13 15:57:48,791 [INFO] [Fold 1 | Epoch 61] Train Loss: 1.12798 | Val Loss: 1.12777 | R²: 0.00990\n",
      "2025-01-13 15:57:48,799 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:01:46,227 [INFO] [Fold 1 | Epoch 62] Train Loss: 1.12699 | Val Loss: 1.12746 | R²: 0.00989\n",
      "2025-01-13 16:01:46,234 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:29:39,582 [INFO] [Fold 1 | Epoch 69] Train Loss: 1.12187 | Val Loss: 1.12494 | R²: 0.01013\n",
      "2025-01-13 16:29:39,591 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:33:39,667 [INFO] [Fold 1 | Epoch 70] Train Loss: 1.12117 | Val Loss: 1.12454 | R²: 0.01021\n",
      "2025-01-13 16:33:39,675 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:37:38,371 [INFO] [Fold 1 | Epoch 71] Train Loss: 1.12033 | Val Loss: 1.12414 | R²: 0.01020\n",
      "2025-01-13 16:37:38,378 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:41:38,623 [INFO] [Fold 1 | Epoch 72] Train Loss: 1.11973 | Val Loss: 1.12374 | R²: 0.01022\n",
      "2025-01-13 16:41:38,630 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:45:39,949 [INFO] [Fold 1 | Epoch 73] Train Loss: 1.11900 | Val Loss: 1.12318 | R²: 0.01031\n",
      "2025-01-13 16:45:39,957 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:49:45,285 [INFO] [Fold 1 | Epoch 74] Train Loss: 1.11816 | Val Loss: 1.12282 | R²: 0.01024\n",
      "2025-01-13 16:49:45,293 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:53:50,665 [INFO] [Fold 1 | Epoch 75] Train Loss: 1.11748 | Val Loss: 1.12235 | R²: 0.01030\n",
      "2025-01-13 16:53:50,672 [INFO] Best model updated for fold 1\n",
      "2025-01-13 16:53:50,674 [INFO] Training Fold 2/3...\n",
      "2025-01-13 17:00:10,834 [INFO] [Fold 2 | Epoch 1] Train Loss: 150.68309 | Val Loss: 135.91602 | R²: -0.00400\n",
      "2025-01-13 17:00:10,841 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:06:25,390 [INFO] [Fold 2 | Epoch 2] Train Loss: 140.57918 | Val Loss: 133.09357 | R²: -0.00034\n",
      "2025-01-13 17:06:25,397 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:12:47,670 [INFO] [Fold 2 | Epoch 3] Train Loss: 138.86858 | Val Loss: 132.25373 | R²: 0.00034\n",
      "2025-01-13 17:12:47,677 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:19:08,671 [INFO] [Fold 2 | Epoch 4] Train Loss: 138.13862 | Val Loss: 131.70765 | R²: 0.00075\n",
      "2025-01-13 17:19:08,678 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:25:31,120 [INFO] [Fold 2 | Epoch 5] Train Loss: 137.55698 | Val Loss: 131.20645 | R²: 0.00105\n",
      "2025-01-13 17:25:31,128 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:31:52,350 [INFO] [Fold 2 | Epoch 6] Train Loss: 137.01013 | Val Loss: 130.71420 | R²: 0.00125\n",
      "2025-01-13 17:31:52,359 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:38:10,737 [INFO] [Fold 2 | Epoch 7] Train Loss: 136.47507 | Val Loss: 130.22463 | R²: 0.00141\n",
      "2025-01-13 17:38:10,745 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:44:36,654 [INFO] [Fold 2 | Epoch 8] Train Loss: 135.94533 | Val Loss: 129.73653 | R²: 0.00151\n",
      "2025-01-13 17:44:36,662 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:51:04,191 [INFO] [Fold 2 | Epoch 9] Train Loss: 135.41911 | Val Loss: 129.24948 | R²: 0.00163\n",
      "2025-01-13 17:51:04,199 [INFO] Best model updated for fold 2\n",
      "2025-01-13 17:57:29,440 [INFO] [Fold 2 | Epoch 10] Train Loss: 134.89512 | Val Loss: 128.76367 | R²: 0.00173\n",
      "2025-01-13 17:57:29,448 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:03:57,372 [INFO] [Fold 2 | Epoch 11] Train Loss: 134.37341 | Val Loss: 128.27899 | R²: 0.00180\n",
      "2025-01-13 18:03:57,379 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:10:23,424 [INFO] [Fold 2 | Epoch 12] Train Loss: 133.85369 | Val Loss: 127.79555 | R²: 0.00188\n",
      "2025-01-13 18:10:23,432 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:16:50,462 [INFO] [Fold 2 | Epoch 13] Train Loss: 133.33549 | Val Loss: 127.31283 | R²: 0.00195\n",
      "2025-01-13 18:16:50,470 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:23:24,321 [INFO] [Fold 2 | Epoch 14] Train Loss: 132.81908 | Val Loss: 126.83118 | R²: 0.00202\n",
      "2025-01-13 18:23:24,328 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:29:51,023 [INFO] [Fold 2 | Epoch 15] Train Loss: 132.30440 | Val Loss: 126.35070 | R²: 0.00210\n",
      "2025-01-13 18:29:51,030 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:36:13,386 [INFO] [Fold 2 | Epoch 16] Train Loss: 131.79108 | Val Loss: 125.87101 | R²: 0.00214\n",
      "2025-01-13 18:36:13,394 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:42:48,281 [INFO] [Fold 2 | Epoch 17] Train Loss: 131.27898 | Val Loss: 125.39238 | R²: 0.00219\n",
      "2025-01-13 18:42:48,289 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:49:23,139 [INFO] [Fold 2 | Epoch 18] Train Loss: 130.76846 | Val Loss: 124.91452 | R²: 0.00226\n",
      "2025-01-13 18:49:23,147 [INFO] Best model updated for fold 2\n",
      "2025-01-13 18:55:56,692 [INFO] [Fold 2 | Epoch 19] Train Loss: 130.25904 | Val Loss: 124.43777 | R²: 0.00233\n",
      "2025-01-13 18:55:56,700 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:02:25,675 [INFO] [Fold 2 | Epoch 20] Train Loss: 129.75061 | Val Loss: 123.96187 | R²: 0.00239\n",
      "2025-01-13 19:02:25,682 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:08:55,961 [INFO] [Fold 2 | Epoch 21] Train Loss: 129.24402 | Val Loss: 123.48691 | R²: 0.00245\n",
      "2025-01-13 19:08:55,969 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:15:22,805 [INFO] [Fold 2 | Epoch 22] Train Loss: 128.73825 | Val Loss: 123.01298 | R²: 0.00253\n",
      "2025-01-13 19:15:22,812 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:21:44,415 [INFO] [Fold 2 | Epoch 23] Train Loss: 128.23363 | Val Loss: 122.53979 | R²: 0.00258\n",
      "2025-01-13 19:21:44,423 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:28:17,527 [INFO] [Fold 2 | Epoch 24] Train Loss: 127.73043 | Val Loss: 122.06785 | R²: 0.00264\n",
      "2025-01-13 19:28:17,535 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:34:43,919 [INFO] [Fold 2 | Epoch 25] Train Loss: 127.22833 | Val Loss: 121.59680 | R²: 0.00272\n",
      "2025-01-13 19:34:43,927 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:41:12,999 [INFO] [Fold 2 | Epoch 26] Train Loss: 126.72741 | Val Loss: 121.12673 | R²: 0.00277\n",
      "2025-01-13 19:41:13,006 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:47:45,223 [INFO] [Fold 2 | Epoch 27] Train Loss: 126.22741 | Val Loss: 120.65750 | R²: 0.00285\n",
      "2025-01-13 19:47:45,231 [INFO] Best model updated for fold 2\n",
      "2025-01-13 19:54:15,293 [INFO] [Fold 2 | Epoch 28] Train Loss: 125.72883 | Val Loss: 120.18930 | R²: 0.00289\n",
      "2025-01-13 19:54:15,301 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:00:41,495 [INFO] [Fold 2 | Epoch 29] Train Loss: 125.23085 | Val Loss: 119.72190 | R²: 0.00297\n",
      "2025-01-13 20:00:41,503 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:07:13,369 [INFO] [Fold 2 | Epoch 30] Train Loss: 124.73419 | Val Loss: 119.25541 | R²: 0.00304\n",
      "2025-01-13 20:07:13,376 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:13:51,238 [INFO] [Fold 2 | Epoch 31] Train Loss: 124.23861 | Val Loss: 118.78992 | R²: 0.00312\n",
      "2025-01-13 20:13:51,246 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:20:31,263 [INFO] [Fold 2 | Epoch 32] Train Loss: 123.74409 | Val Loss: 118.32544 | R²: 0.00318\n",
      "2025-01-13 20:20:31,272 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:27:09,883 [INFO] [Fold 2 | Epoch 33] Train Loss: 123.25076 | Val Loss: 117.86180 | R²: 0.00325\n",
      "2025-01-13 20:27:09,890 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:33:36,974 [INFO] [Fold 2 | Epoch 34] Train Loss: 122.75824 | Val Loss: 117.39905 | R²: 0.00330\n",
      "2025-01-13 20:33:36,981 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:40:12,539 [INFO] [Fold 2 | Epoch 35] Train Loss: 122.26684 | Val Loss: 116.93724 | R²: 0.00339\n",
      "2025-01-13 20:40:12,546 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:46:36,862 [INFO] [Fold 2 | Epoch 36] Train Loss: 121.77645 | Val Loss: 116.47624 | R²: 0.00342\n",
      "2025-01-13 20:46:36,870 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:53:06,915 [INFO] [Fold 2 | Epoch 37] Train Loss: 121.28730 | Val Loss: 116.01638 | R²: 0.00351\n",
      "2025-01-13 20:53:06,923 [INFO] Best model updated for fold 2\n",
      "2025-01-13 20:59:31,428 [INFO] [Fold 2 | Epoch 38] Train Loss: 120.79915 | Val Loss: 115.55721 | R²: 0.00360\n",
      "2025-01-13 20:59:31,435 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:06:02,005 [INFO] [Fold 2 | Epoch 39] Train Loss: 120.31176 | Val Loss: 115.09884 | R²: 0.00365\n",
      "2025-01-13 21:06:02,012 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:12:27,682 [INFO] [Fold 2 | Epoch 40] Train Loss: 119.82572 | Val Loss: 114.64157 | R²: 0.00371\n",
      "2025-01-13 21:12:27,690 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:18:53,841 [INFO] [Fold 2 | Epoch 41] Train Loss: 119.34038 | Val Loss: 114.18506 | R²: 0.00379\n",
      "2025-01-13 21:18:53,849 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:25:24,971 [INFO] [Fold 2 | Epoch 42] Train Loss: 118.85612 | Val Loss: 113.72928 | R²: 0.00385\n",
      "2025-01-13 21:25:24,979 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:31:51,824 [INFO] [Fold 2 | Epoch 43] Train Loss: 118.37226 | Val Loss: 113.27451 | R²: 0.00393\n",
      "2025-01-13 21:31:51,832 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:38:14,723 [INFO] [Fold 2 | Epoch 44] Train Loss: 117.88961 | Val Loss: 112.82079 | R²: 0.00400\n",
      "2025-01-13 21:38:14,730 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:44:40,705 [INFO] [Fold 2 | Epoch 45] Train Loss: 117.40792 | Val Loss: 112.36812 | R²: 0.00408\n",
      "2025-01-13 21:44:40,713 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:51:11,936 [INFO] [Fold 2 | Epoch 46] Train Loss: 116.92757 | Val Loss: 111.91629 | R²: 0.00418\n",
      "2025-01-13 21:51:11,945 [INFO] Best model updated for fold 2\n",
      "2025-01-13 21:57:36,769 [INFO] [Fold 2 | Epoch 47] Train Loss: 116.44839 | Val Loss: 111.46565 | R²: 0.00424\n",
      "2025-01-13 21:57:36,776 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:04:12,546 [INFO] [Fold 2 | Epoch 48] Train Loss: 115.97029 | Val Loss: 111.01606 | R²: 0.00429\n",
      "2025-01-13 22:04:12,554 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:10:43,499 [INFO] [Fold 2 | Epoch 49] Train Loss: 115.49309 | Val Loss: 110.56734 | R²: 0.00440\n",
      "2025-01-13 22:10:43,507 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:17:11,367 [INFO] [Fold 2 | Epoch 50] Train Loss: 115.01708 | Val Loss: 110.11988 | R²: 0.00445\n",
      "2025-01-13 22:17:11,375 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:23:38,024 [INFO] [Fold 2 | Epoch 51] Train Loss: 114.54231 | Val Loss: 109.67303 | R²: 0.00452\n",
      "2025-01-13 22:23:38,032 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:30:05,546 [INFO] [Fold 2 | Epoch 52] Train Loss: 114.06839 | Val Loss: 109.22726 | R²: 0.00462\n",
      "2025-01-13 22:30:05,554 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:36:30,026 [INFO] [Fold 2 | Epoch 53] Train Loss: 113.59566 | Val Loss: 108.78265 | R²: 0.00469\n",
      "2025-01-13 22:36:30,033 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:43:00,212 [INFO] [Fold 2 | Epoch 54] Train Loss: 113.12384 | Val Loss: 108.33886 | R²: 0.00475\n",
      "2025-01-13 22:43:00,220 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:49:33,980 [INFO] [Fold 2 | Epoch 55] Train Loss: 112.65329 | Val Loss: 107.89612 | R²: 0.00485\n",
      "2025-01-13 22:49:33,987 [INFO] Best model updated for fold 2\n",
      "2025-01-13 22:56:01,871 [INFO] [Fold 2 | Epoch 56] Train Loss: 112.18389 | Val Loss: 107.45437 | R²: 0.00489\n",
      "2025-01-13 22:56:01,879 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:02:36,949 [INFO] [Fold 2 | Epoch 57] Train Loss: 111.71547 | Val Loss: 107.01342 | R²: 0.00499\n",
      "2025-01-13 23:02:36,957 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:09:11,499 [INFO] [Fold 2 | Epoch 58] Train Loss: 111.24805 | Val Loss: 106.57363 | R²: 0.00508\n",
      "2025-01-13 23:09:11,507 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:15:46,674 [INFO] [Fold 2 | Epoch 59] Train Loss: 110.78183 | Val Loss: 106.13470 | R²: 0.00516\n",
      "2025-01-13 23:15:46,681 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:22:22,160 [INFO] [Fold 2 | Epoch 60] Train Loss: 110.31659 | Val Loss: 105.69681 | R²: 0.00521\n",
      "2025-01-13 23:22:22,168 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:28:57,615 [INFO] [Fold 2 | Epoch 61] Train Loss: 109.85253 | Val Loss: 105.25984 | R²: 0.00529\n",
      "2025-01-13 23:28:57,623 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:35:34,958 [INFO] [Fold 2 | Epoch 62] Train Loss: 109.38965 | Val Loss: 104.82399 | R²: 0.00534\n",
      "2025-01-13 23:35:34,966 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:42:13,510 [INFO] [Fold 2 | Epoch 63] Train Loss: 108.92761 | Val Loss: 104.38893 | R²: 0.00541\n",
      "2025-01-13 23:42:13,518 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:48:46,874 [INFO] [Fold 2 | Epoch 64] Train Loss: 108.46678 | Val Loss: 103.95502 | R²: 0.00548\n",
      "2025-01-13 23:48:46,881 [INFO] Best model updated for fold 2\n",
      "2025-01-13 23:55:22,520 [INFO] [Fold 2 | Epoch 65] Train Loss: 108.00696 | Val Loss: 103.52206 | R²: 0.00555\n",
      "2025-01-13 23:55:22,528 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:02:01,995 [INFO] [Fold 2 | Epoch 66] Train Loss: 107.54842 | Val Loss: 103.09008 | R²: 0.00563\n",
      "2025-01-14 00:02:02,003 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:08:42,014 [INFO] [Fold 2 | Epoch 67] Train Loss: 107.09101 | Val Loss: 102.65910 | R²: 0.00566\n",
      "2025-01-14 00:08:42,021 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:15:18,278 [INFO] [Fold 2 | Epoch 68] Train Loss: 106.63436 | Val Loss: 102.22894 | R²: 0.00574\n",
      "2025-01-14 00:15:18,286 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:21:52,266 [INFO] [Fold 2 | Epoch 69] Train Loss: 106.17898 | Val Loss: 101.79994 | R²: 0.00581\n",
      "2025-01-14 00:21:52,274 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:28:24,344 [INFO] [Fold 2 | Epoch 70] Train Loss: 105.72457 | Val Loss: 101.37201 | R²: 0.00587\n",
      "2025-01-14 00:28:24,351 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:35:04,608 [INFO] [Fold 2 | Epoch 71] Train Loss: 105.27140 | Val Loss: 100.94485 | R²: 0.00592\n",
      "2025-01-14 00:35:04,616 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:41:42,786 [INFO] [Fold 2 | Epoch 72] Train Loss: 104.81906 | Val Loss: 100.51861 | R²: 0.00599\n",
      "2025-01-14 00:41:42,793 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:48:20,539 [INFO] [Fold 2 | Epoch 73] Train Loss: 104.36784 | Val Loss: 100.09332 | R²: 0.00605\n",
      "2025-01-14 00:48:20,546 [INFO] Best model updated for fold 2\n",
      "2025-01-14 00:54:52,455 [INFO] [Fold 2 | Epoch 74] Train Loss: 103.91760 | Val Loss: 99.66926 | R²: 0.00610\n",
      "2025-01-14 00:54:52,464 [INFO] Best model updated for fold 2\n",
      "2025-01-14 01:01:26,500 [INFO] [Fold 2 | Epoch 75] Train Loss: 103.46846 | Val Loss: 99.24591 | R²: 0.00615\n",
      "2025-01-14 01:01:26,507 [INFO] Best model updated for fold 2\n",
      "2025-01-14 01:01:26,508 [INFO] Training Fold 3/3...\n",
      "2025-01-14 01:10:05,576 [INFO] [Fold 3 | Epoch 1] Train Loss: 153.67671 | Val Loss: 143.03967 | R²: -0.00195\n",
      "2025-01-14 01:10:05,592 [INFO] Best model updated for fold 3\n",
      "2025-01-14 01:18:46,690 [INFO] [Fold 3 | Epoch 2] Train Loss: 144.95296 | Val Loss: 141.37205 | R²: -0.00062\n",
      "2025-01-14 01:18:46,698 [INFO] Best model updated for fold 3\n",
      "2025-01-14 01:27:21,663 [INFO] [Fold 3 | Epoch 3] Train Loss: 143.62501 | Val Loss: 140.62231 | R²: -0.00033\n",
      "2025-01-14 01:27:21,670 [INFO] Best model updated for fold 3\n",
      "2025-01-14 01:36:03,155 [INFO] [Fold 3 | Epoch 4] Train Loss: 142.83323 | Val Loss: 139.91066 | R²: -0.00012\n",
      "2025-01-14 01:36:03,164 [INFO] Best model updated for fold 3\n",
      "2025-01-14 01:44:46,199 [INFO] [Fold 3 | Epoch 5] Train Loss: 142.10134 | Val Loss: 139.20496 | R²: 0.00001\n",
      "2025-01-14 01:44:46,206 [INFO] Best model updated for fold 3\n",
      "2025-01-14 01:53:23,776 [INFO] [Fold 3 | Epoch 6] Train Loss: 141.38467 | Val Loss: 138.50231 | R²: 0.00014\n",
      "2025-01-14 01:53:23,783 [INFO] Best model updated for fold 3\n",
      "2025-01-14 02:02:09,130 [INFO] [Fold 3 | Epoch 7] Train Loss: 140.67526 | Val Loss: 137.80232 | R²: 0.00022\n",
      "2025-01-14 02:02:09,138 [INFO] Best model updated for fold 3\n",
      "2025-01-14 02:10:53,850 [INFO] [Fold 3 | Epoch 8] Train Loss: 139.96995 | Val Loss: 137.10474 | R²: 0.00031\n",
      "2025-01-14 02:10:53,857 [INFO] Best model updated for fold 3\n",
      "2025-01-14 02:19:29,906 [INFO] [Fold 3 | Epoch 9] Train Loss: 139.26877 | Val Loss: 136.40967 | R²: 0.00037\n",
      "2025-01-14 02:19:29,914 [INFO] Best model updated for fold 3\n",
      "2025-01-14 02:28:20,706 [INFO] [Fold 3 | Epoch 10] Train Loss: 138.57036 | Val Loss: 135.71704 | R²: 0.00043\n",
      "2025-01-14 02:28:20,714 [INFO] Best model updated for fold 3\n",
      "2025-01-14 02:36:58,439 [INFO] [Fold 3 | Epoch 11] Train Loss: 137.87481 | Val Loss: 135.02648 | R²: 0.00047\n",
      "2025-01-14 02:36:58,447 [INFO] Best model updated for fold 3\n",
      "2025-01-14 02:45:37,043 [INFO] [Fold 3 | Epoch 12] Train Loss: 137.18153 | Val Loss: 134.33800 | R²: 0.00052\n",
      "2025-01-14 02:45:37,052 [INFO] Best model updated for fold 3\n",
      "2025-01-14 02:54:26,446 [INFO] [Fold 3 | Epoch 13] Train Loss: 136.49029 | Val Loss: 133.65156 | R²: 0.00056\n",
      "2025-01-14 02:54:26,454 [INFO] Best model updated for fold 3\n",
      "2025-01-14 03:03:10,105 [INFO] [Fold 3 | Epoch 14] Train Loss: 135.80159 | Val Loss: 132.96717 | R²: 0.00059\n",
      "2025-01-14 03:03:10,113 [INFO] Best model updated for fold 3\n",
      "2025-01-14 03:11:53,700 [INFO] [Fold 3 | Epoch 15] Train Loss: 135.11428 | Val Loss: 132.28458 | R²: 0.00063\n",
      "2025-01-14 03:11:53,707 [INFO] Best model updated for fold 3\n",
      "2025-01-14 03:20:40,062 [INFO] [Fold 3 | Epoch 16] Train Loss: 134.42969 | Val Loss: 131.60411 | R²: 0.00066\n",
      "2025-01-14 03:20:40,069 [INFO] Best model updated for fold 3\n",
      "2025-01-14 03:29:29,541 [INFO] [Fold 3 | Epoch 17] Train Loss: 133.74674 | Val Loss: 130.92552 | R²: 0.00068\n",
      "2025-01-14 03:29:29,548 [INFO] Best model updated for fold 3\n",
      "2025-01-14 03:38:14,469 [INFO] [Fold 3 | Epoch 18] Train Loss: 133.06568 | Val Loss: 130.24877 | R²: 0.00071\n",
      "2025-01-14 03:38:14,477 [INFO] Best model updated for fold 3\n",
      "2025-01-14 03:47:06,392 [INFO] [Fold 3 | Epoch 19] Train Loss: 132.38670 | Val Loss: 129.57389 | R²: 0.00075\n",
      "2025-01-14 03:47:06,399 [INFO] Best model updated for fold 3\n",
      "2025-01-14 03:55:55,097 [INFO] [Fold 3 | Epoch 20] Train Loss: 131.70930 | Val Loss: 128.90068 | R²: 0.00078\n",
      "2025-01-14 03:55:55,105 [INFO] Best model updated for fold 3\n",
      "2025-01-14 04:04:47,339 [INFO] [Fold 3 | Epoch 21] Train Loss: 131.03373 | Val Loss: 128.22936 | R²: 0.00081\n",
      "2025-01-14 04:04:47,346 [INFO] Best model updated for fold 3\n",
      "2025-01-14 04:13:41,662 [INFO] [Fold 3 | Epoch 22] Train Loss: 130.35991 | Val Loss: 127.55960 | R²: 0.00085\n",
      "2025-01-14 04:13:41,669 [INFO] Best model updated for fold 3\n",
      "2025-01-14 04:22:27,948 [INFO] [Fold 3 | Epoch 23] Train Loss: 129.68801 | Val Loss: 126.89174 | R²: 0.00089\n",
      "2025-01-14 04:22:27,956 [INFO] Best model updated for fold 3\n",
      "2025-01-14 04:31:15,389 [INFO] [Fold 3 | Epoch 24] Train Loss: 129.01762 | Val Loss: 126.22534 | R²: 0.00092\n",
      "2025-01-14 04:31:15,398 [INFO] Best model updated for fold 3\n",
      "2025-01-14 04:40:02,221 [INFO] [Fold 3 | Epoch 25] Train Loss: 128.34923 | Val Loss: 125.56062 | R²: 0.00096\n",
      "2025-01-14 04:40:02,230 [INFO] Best model updated for fold 3\n",
      "2025-01-14 04:48:49,974 [INFO] [Fold 3 | Epoch 26] Train Loss: 127.68222 | Val Loss: 124.89744 | R²: 0.00100\n",
      "2025-01-14 04:48:49,982 [INFO] Best model updated for fold 3\n",
      "2025-01-14 04:57:38,139 [INFO] [Fold 3 | Epoch 27] Train Loss: 127.01741 | Val Loss: 124.23562 | R²: 0.00102\n",
      "2025-01-14 04:57:38,148 [INFO] Best model updated for fold 3\n",
      "2025-01-14 05:06:25,077 [INFO] [Fold 3 | Epoch 28] Train Loss: 126.35388 | Val Loss: 123.57548 | R²: 0.00106\n",
      "2025-01-14 05:06:25,085 [INFO] Best model updated for fold 3\n",
      "2025-01-14 05:15:10,098 [INFO] [Fold 3 | Epoch 29] Train Loss: 125.69200 | Val Loss: 122.91692 | R²: 0.00107\n",
      "2025-01-14 05:15:10,105 [INFO] Best model updated for fold 3\n",
      "2025-01-14 05:23:58,326 [INFO] [Fold 3 | Epoch 30] Train Loss: 125.03237 | Val Loss: 122.26080 | R²: 0.00110\n",
      "2025-01-14 05:23:58,335 [INFO] Best model updated for fold 3\n",
      "2025-01-14 05:32:47,282 [INFO] [Fold 3 | Epoch 31] Train Loss: 124.37460 | Val Loss: 121.60699 | R²: 0.00112\n",
      "2025-01-14 05:32:47,289 [INFO] Best model updated for fold 3\n",
      "2025-01-14 05:41:34,884 [INFO] [Fold 3 | Epoch 32] Train Loss: 123.71892 | Val Loss: 120.95561 | R²: 0.00114\n",
      "2025-01-14 05:41:34,891 [INFO] Best model updated for fold 3\n",
      "2025-01-14 05:50:22,331 [INFO] [Fold 3 | Epoch 33] Train Loss: 123.06526 | Val Loss: 120.30619 | R²: 0.00117\n",
      "2025-01-14 05:50:22,339 [INFO] Best model updated for fold 3\n",
      "2025-01-14 05:59:11,915 [INFO] [Fold 3 | Epoch 34] Train Loss: 122.41365 | Val Loss: 119.65911 | R²: 0.00120\n",
      "2025-01-14 05:59:11,923 [INFO] Best model updated for fold 3\n",
      "2025-01-14 06:07:58,582 [INFO] [Fold 3 | Epoch 35] Train Loss: 121.76384 | Val Loss: 119.01385 | R²: 0.00124\n",
      "2025-01-14 06:07:58,590 [INFO] Best model updated for fold 3\n",
      "2025-01-14 06:16:48,767 [INFO] [Fold 3 | Epoch 36] Train Loss: 121.11608 | Val Loss: 118.37056 | R²: 0.00125\n",
      "2025-01-14 06:16:48,774 [INFO] Best model updated for fold 3\n",
      "2025-01-14 06:25:39,883 [INFO] [Fold 3 | Epoch 37] Train Loss: 120.47016 | Val Loss: 117.72931 | R²: 0.00129\n",
      "2025-01-14 06:25:39,891 [INFO] Best model updated for fold 3\n",
      "2025-01-14 06:34:29,193 [INFO] [Fold 3 | Epoch 38] Train Loss: 119.82612 | Val Loss: 117.08988 | R²: 0.00132\n",
      "2025-01-14 06:34:29,201 [INFO] Best model updated for fold 3\n",
      "2025-01-14 06:43:19,546 [INFO] [Fold 3 | Epoch 39] Train Loss: 119.18404 | Val Loss: 116.45235 | R²: 0.00136\n",
      "2025-01-14 06:43:19,553 [INFO] Best model updated for fold 3\n",
      "2025-01-14 06:52:06,006 [INFO] [Fold 3 | Epoch 40] Train Loss: 118.54387 | Val Loss: 115.81687 | R²: 0.00139\n",
      "2025-01-14 06:52:06,014 [INFO] Best model updated for fold 3\n",
      "2025-01-14 07:00:45,175 [INFO] [Fold 3 | Epoch 41] Train Loss: 117.90550 | Val Loss: 115.18321 | R²: 0.00142\n",
      "2025-01-14 07:00:45,183 [INFO] Best model updated for fold 3\n",
      "2025-01-14 07:09:26,476 [INFO] [Fold 3 | Epoch 42] Train Loss: 117.26917 | Val Loss: 114.55159 | R²: 0.00146\n",
      "2025-01-14 07:09:26,485 [INFO] Best model updated for fold 3\n",
      "2025-01-14 07:18:06,733 [INFO] [Fold 3 | Epoch 43] Train Loss: 116.63461 | Val Loss: 113.92181 | R²: 0.00148\n",
      "2025-01-14 07:18:06,741 [INFO] Best model updated for fold 3\n",
      "2025-01-14 07:26:47,741 [INFO] [Fold 3 | Epoch 44] Train Loss: 116.00202 | Val Loss: 113.29383 | R²: 0.00150\n",
      "2025-01-14 07:26:47,749 [INFO] Best model updated for fold 3\n",
      "2025-01-14 07:35:26,037 [INFO] [Fold 3 | Epoch 45] Train Loss: 115.37126 | Val Loss: 112.66782 | R²: 0.00154\n",
      "2025-01-14 07:35:26,045 [INFO] Best model updated for fold 3\n",
      "2025-01-14 07:44:02,618 [INFO] [Fold 3 | Epoch 46] Train Loss: 114.74216 | Val Loss: 112.04356 | R²: 0.00157\n",
      "2025-01-14 07:44:02,626 [INFO] Best model updated for fold 3\n",
      "2025-01-14 07:53:02,681 [INFO] [Fold 3 | Epoch 47] Train Loss: 114.11516 | Val Loss: 111.42135 | R²: 0.00158\n",
      "2025-01-14 07:53:02,689 [INFO] Best model updated for fold 3\n",
      "2025-01-14 08:02:16,797 [INFO] [Fold 3 | Epoch 48] Train Loss: 113.49019 | Val Loss: 110.80087 | R²: 0.00162\n",
      "2025-01-14 08:02:16,805 [INFO] Best model updated for fold 3\n",
      "2025-01-14 08:11:35,407 [INFO] [Fold 3 | Epoch 49] Train Loss: 112.86673 | Val Loss: 110.18240 | R²: 0.00165\n",
      "2025-01-14 08:11:35,415 [INFO] Best model updated for fold 3\n",
      "2025-01-14 08:20:28,040 [INFO] [Fold 3 | Epoch 50] Train Loss: 112.24527 | Val Loss: 109.56583 | R²: 0.00168\n",
      "2025-01-14 08:20:28,048 [INFO] Best model updated for fold 3\n",
      "2025-01-14 08:29:48,081 [INFO] [Fold 3 | Epoch 51] Train Loss: 111.62575 | Val Loss: 108.95111 | R²: 0.00170\n",
      "2025-01-14 08:29:48,090 [INFO] Best model updated for fold 3\n",
      "2025-01-14 08:39:21,814 [INFO] [Fold 3 | Epoch 52] Train Loss: 111.00813 | Val Loss: 108.33824 | R²: 0.00174\n",
      "2025-01-14 08:39:21,822 [INFO] Best model updated for fold 3\n",
      "2025-01-14 08:48:58,727 [INFO] [Fold 3 | Epoch 53] Train Loss: 110.39221 | Val Loss: 107.72723 | R²: 0.00176\n",
      "2025-01-14 08:48:58,735 [INFO] Best model updated for fold 3\n",
      "2025-01-14 08:58:35,145 [INFO] [Fold 3 | Epoch 54] Train Loss: 109.77808 | Val Loss: 107.11814 | R²: 0.00179\n",
      "2025-01-14 08:58:35,154 [INFO] Best model updated for fold 3\n",
      "2025-01-14 09:08:07,352 [INFO] [Fold 3 | Epoch 55] Train Loss: 109.16589 | Val Loss: 106.51092 | R²: 0.00180\n",
      "2025-01-14 09:08:07,361 [INFO] Best model updated for fold 3\n",
      "2025-01-14 09:18:03,978 [INFO] [Fold 3 | Epoch 56] Train Loss: 108.55553 | Val Loss: 105.90553 | R²: 0.00183\n",
      "2025-01-14 09:18:03,986 [INFO] Best model updated for fold 3\n",
      "2025-01-14 09:27:40,850 [INFO] [Fold 3 | Epoch 57] Train Loss: 107.94720 | Val Loss: 105.30216 | R²: 0.00185\n",
      "2025-01-14 09:27:40,861 [INFO] Best model updated for fold 3\n",
      "2025-01-14 09:37:12,534 [INFO] [Fold 3 | Epoch 58] Train Loss: 107.34070 | Val Loss: 104.70058 | R²: 0.00188\n",
      "2025-01-14 09:37:12,543 [INFO] Best model updated for fold 3\n",
      "2025-01-14 09:46:42,650 [INFO] [Fold 3 | Epoch 59] Train Loss: 106.73597 | Val Loss: 104.10084 | R²: 0.00190\n",
      "2025-01-14 09:46:42,658 [INFO] Best model updated for fold 3\n",
      "2025-01-14 09:55:56,455 [INFO] [Fold 3 | Epoch 60] Train Loss: 106.13294 | Val Loss: 103.50301 | R²: 0.00191\n",
      "2025-01-14 09:55:56,463 [INFO] Best model updated for fold 3\n",
      "2025-01-14 10:05:48,404 [INFO] [Fold 3 | Epoch 61] Train Loss: 105.53209 | Val Loss: 102.90707 | R²: 0.00194\n",
      "2025-01-14 10:05:48,413 [INFO] Best model updated for fold 3\n",
      "2025-01-14 10:15:30,749 [INFO] [Fold 3 | Epoch 62] Train Loss: 104.93263 | Val Loss: 102.31287 | R²: 0.00194\n",
      "2025-01-14 10:15:30,757 [INFO] Best model updated for fold 3\n",
      "2025-01-14 10:24:42,466 [INFO] [Fold 3 | Epoch 63] Train Loss: 104.33538 | Val Loss: 101.72053 | R²: 0.00198\n",
      "2025-01-14 10:24:42,474 [INFO] Best model updated for fold 3\n",
      "2025-01-14 10:33:50,559 [INFO] [Fold 3 | Epoch 64] Train Loss: 103.73976 | Val Loss: 101.13009 | R²: 0.00200\n",
      "2025-01-14 10:33:50,567 [INFO] Best model updated for fold 3\n",
      "2025-01-14 10:42:47,921 [INFO] [Fold 3 | Epoch 65] Train Loss: 103.14610 | Val Loss: 100.54155 | R²: 0.00203\n",
      "2025-01-14 10:42:47,930 [INFO] Best model updated for fold 3\n",
      "2025-01-14 10:51:40,643 [INFO] [Fold 3 | Epoch 66] Train Loss: 102.55420 | Val Loss: 99.95490 | R²: 0.00204\n",
      "2025-01-14 10:51:40,652 [INFO] Best model updated for fold 3\n",
      "2025-01-14 11:01:11,275 [INFO] [Fold 3 | Epoch 67] Train Loss: 101.94781 | Val Loss: 99.29864 | R²: 0.00208\n",
      "2025-01-14 11:01:11,283 [INFO] Best model updated for fold 3\n",
      "2025-01-14 11:10:40,077 [INFO] [Fold 3 | Epoch 68] Train Loss: 101.20751 | Val Loss: 98.52673 | R²: 0.00210\n",
      "2025-01-14 11:10:40,087 [INFO] Best model updated for fold 3\n",
      "2025-01-14 11:20:19,581 [INFO] [Fold 3 | Epoch 69] Train Loss: 100.43045 | Val Loss: 97.75808 | R²: 0.00211\n",
      "2025-01-14 11:20:19,589 [INFO] Best model updated for fold 3\n",
      "2025-01-14 11:29:51,584 [INFO] [Fold 3 | Epoch 70] Train Loss: 99.65643 | Val Loss: 96.99263 | R²: 0.00213\n",
      "2025-01-14 11:29:51,592 [INFO] Best model updated for fold 3\n",
      "2025-01-14 11:39:25,010 [INFO] [Fold 3 | Epoch 71] Train Loss: 98.88602 | Val Loss: 96.23036 | R²: 0.00213\n",
      "2025-01-14 11:39:25,018 [INFO] Best model updated for fold 3\n",
      "2025-01-14 11:48:53,351 [INFO] [Fold 3 | Epoch 72] Train Loss: 98.11858 | Val Loss: 95.47140 | R²: 0.00216\n",
      "2025-01-14 11:48:53,359 [INFO] Best model updated for fold 3\n",
      "2025-01-14 11:58:19,977 [INFO] [Fold 3 | Epoch 73] Train Loss: 97.35442 | Val Loss: 94.71565 | R²: 0.00218\n",
      "2025-01-14 11:58:19,985 [INFO] Best model updated for fold 3\n",
      "2025-01-14 12:07:49,236 [INFO] [Fold 3 | Epoch 74] Train Loss: 96.59323 | Val Loss: 93.96321 | R²: 0.00219\n",
      "2025-01-14 12:07:49,244 [INFO] Best model updated for fold 3\n",
      "2025-01-14 12:17:37,078 [INFO] [Fold 3 | Epoch 75] Train Loss: 95.83543 | Val Loss: 93.21397 | R²: 0.00220\n",
      "2025-01-14 12:17:37,085 [INFO] Best model updated for fold 3\n"
     ]
    }
   ],
   "source": [
    "#current\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score\n",
    "import logging\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Reproducibility Setup\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False  # Safer for reproducibility\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Constants\n",
    "HIDDEN_DIM = 512\n",
    "DROPOUT_RATE = 0.2\n",
    "NOISE_STD = 0.001\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 256\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 75\n",
    "N_FOLDS = 5\n",
    "GAP_SIZE = 30\n",
    "VAL_SIZE = 100\n",
    "TRAIN_SIZE = 300\n",
    "GRADIENT_CLIP_VALUE = 0.5\n",
    "LR_PATIENCE = 5\n",
    "MIN_LR = 1e-7\n",
    "\n",
    "# Paths\n",
    "PARTITIONED_DIR = \"/home/jupyter/data/partitioned/\"\n",
    "all_part_files = sorted(glob.glob(os.path.join(PARTITIONED_DIR, \"*.parquet\")))\n",
    "logger.info(f\"Found {len(all_part_files)} partitioned files.\")\n",
    "\n",
    "# Exclude columns\n",
    "EXCLUDE_COLS = [\"date_id\", \"time_id\", \"symbol_id\", \"weight\", \"partition_id\"]\n",
    "df_tmp = pl.read_parquet(all_part_files[0])\n",
    "EXCLUDE_COLS += [col for col in df_tmp.columns if col.startswith(\"responder_\")]\n",
    "del df_tmp\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, features, target, weights):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx], self.weights[idx]\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.05):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.std > 0:\n",
    "            return x + torch.randn_like(x) * self.std\n",
    "        return x\n",
    "\n",
    "class AutoencoderWithMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, dropout_rate=0.2, noise_std=0.001):  # Note original values\n",
    "        super().__init__()\n",
    "        self.noise = GaussianNoise(std=noise_std)\n",
    "        \n",
    "        # Original dimensionality reduction that worked\n",
    "        dim1 = hidden_dim // 2  # 256\n",
    "        dim2 = hidden_dim // 4  # 128\n",
    "        dim3 = hidden_dim // 8  # 64\n",
    "        \n",
    "        self.input_norm = nn.LayerNorm(input_dim)  # Keep LayerNorm\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim1),\n",
    "            nn.LayerNorm(dim1),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(dim1, dim2),\n",
    "            nn.LayerNorm(dim2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(dim2, dim3),\n",
    "            nn.LayerNorm(dim3)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim3, dim2),\n",
    "            nn.LayerNorm(dim2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(dim2, input_dim),\n",
    "            nn.LayerNorm(input_dim)\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim + dim3, dim1),\n",
    "            nn.LayerNorm(dim1),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(dim1, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.1, mode='fan_in', nonlinearity='linear')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_norm(x)\n",
    "        x_noised = self.noise(x)\n",
    "        \n",
    "        encoded = self.encoder(x_noised)\n",
    "        latent = self.bottleneck(encoded)\n",
    "        \n",
    "        reconstructed = self.decoder(latent)\n",
    "        reconstructed = reconstructed + x  # Skip connection from original\n",
    "        \n",
    "        mlp_input = torch.cat([x, latent], dim=1)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        \n",
    "        return reconstructed, mlp_output, latent\n",
    "\n",
    "import math\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"Initialize weights using Kaiming He initialization with manual gain for SiLU.\"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # Manual gain for SiLU (approximated)\n",
    "            gain = math.sqrt(2)\n",
    "            nn.init.kaiming_normal_(m.weight, a=gain, nonlinearity=\"leaky_relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def get_unique_dates(files):\n",
    "    all_dates = set()\n",
    "    for file in files:\n",
    "        df = pl.read_parquet(file).select(\"date_id\")\n",
    "        unique_dates = df.unique().to_series().to_list()\n",
    "        all_dates.update(unique_dates)\n",
    "    return sorted(all_dates)\n",
    "\n",
    "def load_and_scale_data_for_dates(\n",
    "    parquet_file, date_list, feature_scaler, exclude_cols, \n",
    "    target_col=\"responder_6\", weight_col=\"weight\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a Parquet file, filter rows to only those date_ids in date_list,\n",
    "    apply scaling, and return (features, targets, weights).\n",
    "    If no rows match, or all are dropped as null, returns (None, None, None).\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(parquet_file)\n",
    "    df = df.filter(pl.col(\"date_id\").is_in(date_list))\n",
    "    if df.is_empty():\n",
    "        return None, None, None\n",
    "\n",
    "    numeric_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    needed_cols = numeric_cols + [target_col, weight_col]\n",
    "\n",
    "    # Drop any rows containing null in these columns\n",
    "    df = df.drop_nulls(subset=needed_cols)\n",
    "    if df.is_empty():\n",
    "        return None, None, None\n",
    "\n",
    "    features = df.select(numeric_cols).to_numpy().astype(np.float32)\n",
    "    targets = df[target_col].to_numpy().astype(np.float32)\n",
    "    weights = df[weight_col].to_numpy().astype(np.float32)\n",
    "\n",
    "    # Scale features and ensure no NaNs remain\n",
    "    features = feature_scaler.transform(features)\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    if np.any(np.abs(features) > 100):\n",
    "        features = np.clip(features, -100, 100)\n",
    "    # Normalize targets (standardization)\n",
    "    targets_mean = targets.mean()\n",
    "    targets_std = targets.std() + 1e-6  # Avoid division by zero\n",
    "    targets = (targets - targets_mean) / targets_std  # <- Added normalization\n",
    "    \n",
    "    return features, targets, weights\n",
    "\n",
    "def create_rolling_folds(dates_list, n_folds, gap_size, val_size, train_size):\n",
    "    folds = []\n",
    "    idx_start = 0\n",
    "    total_dates = len(dates_list)\n",
    "\n",
    "    for _ in range(n_folds):\n",
    "        train_end = idx_start + train_size\n",
    "        gap_end = train_end + gap_size\n",
    "        val_end = gap_end + val_size\n",
    "\n",
    "        if val_end > total_dates:\n",
    "            break\n",
    "\n",
    "        train_dates = dates_list[idx_start:train_end]\n",
    "        val_dates = dates_list[gap_end:val_end]\n",
    "        \n",
    "        folds.append((train_dates, val_dates))\n",
    "        idx_start = val_end\n",
    "\n",
    "    return folds\n",
    "\n",
    "# Build or load the scaler\n",
    "if os.path.exists(\"feature_scaler.pkl\"):\n",
    "    with open(\"feature_scaler.pkl\", \"rb\") as f:\n",
    "        feature_scaler = pickle.load(f)\n",
    "else:\n",
    "    df_tmp = pl.read_parquet(all_part_files[0])\n",
    "    numeric_cols = [col for col in df_tmp.columns if col not in EXCLUDE_COLS]\n",
    "    features = df_tmp.select(numeric_cols).to_numpy()\n",
    "    feature_scaler = StandardScaler().fit(features)\n",
    "    with open(\"feature_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(feature_scaler, f)\n",
    "    del df_tmp\n",
    "\n",
    "GLOBAL_INPUT_DIM = feature_scaler.n_features_in_\n",
    "sorted_dates = get_unique_dates(all_part_files)\n",
    "folds = create_rolling_folds(sorted_dates, N_FOLDS, GAP_SIZE, VAL_SIZE, TRAIN_SIZE)\n",
    "\n",
    "logger.info(f\"Created {len(folds)} folds\")\n",
    "\n",
    "def train_model_with_cv(folds, files, feature_scaler):\n",
    "    for fold_idx, (train_dates, val_dates) in enumerate(folds, start=1):\n",
    "        logger.info(f\"Training Fold {fold_idx}/{len(folds)}...\")\n",
    "\n",
    "        model = AutoencoderWithMLP(\n",
    "            input_dim=GLOBAL_INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            dropout_rate=DROPOUT_RATE,\n",
    "            noise_std=NOISE_STD\n",
    "        )\n",
    "        initialize_weights(model)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=LEARNING_RATE,  # Keep original 1e-5\n",
    "            weight_decay=WEIGHT_DECAY,  # Keep original 1e-5\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=LEARNING_RATE,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=len(train_loader) if 'train_loader' in locals() else 100,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos',\n",
    "            div_factor=25.0  # Starts with lr/25\n",
    "        )\n",
    "\n",
    "        recon_criterion = nn.MSELoss()\n",
    "        supervised_criterion = nn.MSELoss()\n",
    "        scaler = torch.amp.GradScaler()\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=7)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            # ----------------------------\n",
    "            # Training Phase\n",
    "            # ----------------------------\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_steps = 0\n",
    "\n",
    "            for file in files:\n",
    "                features, targets, weights = load_and_scale_data_for_dates(\n",
    "                    file, train_dates, feature_scaler, EXCLUDE_COLS\n",
    "                )\n",
    "                if features is None:\n",
    "                    continue\n",
    "\n",
    "                train_dataset = FinancialDataset(features, targets, weights)\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                    num_workers=4, pin_memory=True\n",
    "                )\n",
    "\n",
    "                for batch_x, batch_y, batch_w in train_loader:\n",
    "                    batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                        reconstructed, mlp_output, _ = model(batch_x)\n",
    "                        loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                        loss_sup = supervised_criterion(mlp_output.squeeze(), batch_y)\n",
    "                        # Safer weighted loss to avoid NaN issues\n",
    "                        loss_sup = (loss_sup * batch_w).sum() / (batch_w.sum() + 1e-6)\n",
    "                        loss = loss_recon + loss_sup\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    train_steps += 1\n",
    "\n",
    "            avg_train_loss = train_loss / train_steps if train_steps > 0 else float('nan')\n",
    "\n",
    "            # ----------------------------\n",
    "            # Validation Phase\n",
    "            # ----------------------------\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_steps = 0\n",
    "            all_targets, all_predictions = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for file in files:\n",
    "                    features, targets, weights = load_and_scale_data_for_dates(\n",
    "                        file, val_dates, feature_scaler, EXCLUDE_COLS\n",
    "                    )\n",
    "                    if features is None:\n",
    "                        continue\n",
    "\n",
    "                    val_dataset = FinancialDataset(features, targets, weights)\n",
    "                    val_loader = DataLoader(\n",
    "                        val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=4, pin_memory=True\n",
    "                    )\n",
    "\n",
    "                    for batch_x, batch_y, batch_w in val_loader:\n",
    "                        batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "\n",
    "                        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "\n",
    "                            reconstructed, mlp_output, _ = model(batch_x)\n",
    "                            loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                            loss_sup = supervised_criterion(mlp_output.squeeze(), batch_y)\n",
    "                            loss_sup = (loss_sup * batch_w).sum() / (batch_w.sum() + 1e-6)\n",
    "                            loss = loss_recon + loss_sup\n",
    "\n",
    "                        val_loss += loss.item()\n",
    "                        val_steps += 1\n",
    "\n",
    "                        # Collect predictions and targets for R² calculation\n",
    "                        all_targets.extend(batch_y.cpu().numpy())\n",
    "                        all_predictions.extend(mlp_output.squeeze().cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / val_steps if val_steps > 0 else float('nan')\n",
    "\n",
    "            # R² calculation with NaN protection\n",
    "            all_targets = np.array(all_targets)\n",
    "            all_predictions = np.array(all_predictions)\n",
    "            valid_mask = np.isfinite(all_targets) & np.isfinite(all_predictions)\n",
    "            filtered_targets = all_targets[valid_mask]\n",
    "            filtered_predictions = all_predictions[valid_mask]\n",
    "            r2 = r2_score(filtered_targets, filtered_predictions) if len(filtered_targets) > 1 else float('nan')\n",
    "\n",
    "            logger.info(f\"[Fold {fold_idx} | Epoch {epoch}] \"\n",
    "                        f\"Train Loss: {avg_train_loss:.5f} | \"\n",
    "                        f\"Val Loss: {avg_val_loss:.5f} | \"\n",
    "                        f\"R²: {r2:.5f}\")\n",
    "\n",
    "            # ----------------------------\n",
    "            # Save the best model\n",
    "            # ----------------------------\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), f\"best_model_fold_{fold_idx}.pt\")\n",
    "                logger.info(f\"Best model updated for fold {fold_idx}\")\n",
    "\n",
    "            # Step scheduler and early stopping\n",
    "            scheduler.step()\n",
    "            early_stopping(avg_val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(f\"Early stopping triggered for Fold {fold_idx}\")\n",
    "                break\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Main Execution (unchanged from original)\n",
    "# ----------------------------------------------------------------------------\n",
    "sorted_dates = get_unique_dates(all_part_files)\n",
    "folds = create_rolling_folds(sorted_dates, N_FOLDS, GAP_SIZE, VAL_SIZE, TRAIN_SIZE)\n",
    "logger.info(f\"Created {len(folds)} folds\")\n",
    "\n",
    "train_model_with_cv(folds, all_part_files, feature_scaler)\n",
    "#current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9afd4c-571a-443c-84ae-1b00c501928b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_fold_data(fold_idx, train_dates, val_dates, files, feature_scaler, exclude_cols):\n",
    "    \"\"\"Analyze data distribution and statistics for each fold.\"\"\"\n",
    "    train_stats = defaultdict(list)\n",
    "    val_stats = defaultdict(list)\n",
    "    \n",
    "    logger.info(f\"\\nAnalyzing Fold {fold_idx}:\")\n",
    "    logger.info(f\"Train dates range: {min(train_dates)} to {max(train_dates)}\")\n",
    "    logger.info(f\"Val dates range: {min(val_dates)} to {max(val_dates)}\")\n",
    "    \n",
    "    total_train_samples = 0\n",
    "    total_val_samples = 0\n",
    "    \n",
    "    for file in files:\n",
    "        # Analyze training data\n",
    "        train_features, train_targets, train_weights = load_and_scale_data_for_dates(\n",
    "            file, train_dates, feature_scaler, exclude_cols\n",
    "        )\n",
    "        \n",
    "        if train_features is not None:\n",
    "            total_train_samples += len(train_features)\n",
    "            train_stats['feature_means'].append(np.nanmean(train_features, axis=0))\n",
    "            train_stats['feature_stds'].append(np.nanstd(train_features, axis=0))\n",
    "            train_stats['target_mean'].append(np.nanmean(train_targets))\n",
    "            train_stats['target_std'].append(np.nanstd(train_targets))\n",
    "            train_stats['weight_mean'].append(np.nanmean(train_weights))\n",
    "            train_stats['nan_counts'].append(np.isnan(train_features).sum())\n",
    "        \n",
    "        # Analyze validation data\n",
    "        val_features, val_targets, val_weights = load_and_scale_data_for_dates(\n",
    "            file, val_dates, feature_scaler, exclude_cols\n",
    "        )\n",
    "        \n",
    "        if val_features is not None:\n",
    "            total_val_samples += len(val_features)\n",
    "            val_stats['feature_means'].append(np.nanmean(val_features, axis=0))\n",
    "            val_stats['feature_stds'].append(np.nanstd(val_features, axis=0))\n",
    "            val_stats['target_mean'].append(np.nanmean(val_targets))\n",
    "            val_stats['target_std'].append(np.nanstd(val_targets))\n",
    "            val_stats['weight_mean'].append(np.nanmean(val_weights))\n",
    "            val_stats['nan_counts'].append(np.isnan(val_features).sum())\n",
    "    \n",
    "    # Log statistics\n",
    "    logger.info(f\"Total training samples: {total_train_samples}\")\n",
    "    logger.info(f\"Total validation samples: {total_val_samples}\")\n",
    "    \n",
    "    if train_stats['feature_means']:\n",
    "        logger.info(\"\\nTraining Set Statistics:\")\n",
    "        logger.info(f\"Average feature means: {np.nanmean(train_stats['feature_means']):.4f}\")\n",
    "        logger.info(f\"Average feature stds: {np.nanmean(train_stats['feature_stds']):.4f}\")\n",
    "        logger.info(f\"Target mean: {np.nanmean(train_stats['target_mean']):.4f}\")\n",
    "        logger.info(f\"Target std: {np.nanmean(train_stats['target_std']):.4f}\")\n",
    "        logger.info(f\"Weight mean: {np.nanmean(train_stats['weight_mean']):.4f}\")\n",
    "        logger.info(f\"Total NaN count: {sum(train_stats['nan_counts'])}\")\n",
    "    \n",
    "    if val_stats['feature_means']:\n",
    "        logger.info(\"\\nValidation Set Statistics:\")\n",
    "        logger.info(f\"Average feature means: {np.nanmean(val_stats['feature_means']):.4f}\")\n",
    "        logger.info(f\"Average feature stds: {np.nanmean(val_stats['feature_stds']):.4f}\")\n",
    "        logger.info(f\"Target mean: {np.nanmean(val_stats['target_mean']):.4f}\")\n",
    "        logger.info(f\"Target std: {np.nanmean(val_stats['target_std']):.4f}\")\n",
    "        logger.info(f\"Weight mean: {np.nanmean(val_stats['weight_mean']):.4f}\")\n",
    "        logger.info(f\"Total NaN count: {sum(val_stats['nan_counts'])}\")\n",
    "    \n",
    "    return total_train_samples, total_val_samples\n",
    "\n",
    "def enhanced_train_model_with_cv(folds, files, feature_scaler):\n",
    "    \"\"\"Enhanced training function with additional debugging and safety checks.\"\"\"\n",
    "    for fold_idx, (train_dates, val_dates) in enumerate(folds, start=1):\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Starting Fold {fold_idx}/{len(folds)}\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "        \n",
    "        # Analyze fold data before training\n",
    "        total_train_samples, total_val_samples = analyze_fold_data(\n",
    "            fold_idx, train_dates, val_dates, files, feature_scaler, EXCLUDE_COLS\n",
    "        )\n",
    "        \n",
    "        if total_train_samples < 1000 or total_val_samples < 100:\n",
    "            logger.warning(f\"Insufficient data in fold {fold_idx}. \"\n",
    "                         f\"Training samples: {total_train_samples}, \"\n",
    "                         f\"Validation samples: {total_val_samples}\")\n",
    "            continue\n",
    "            \n",
    "        # Model and optimizer setup\n",
    "        model = AutoencoderWithMLP(\n",
    "            input_dim=GLOBAL_INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            dropout_rate=DROPOUT_RATE,\n",
    "            noise_std=NOISE_STD\n",
    "        )\n",
    "        initialize_weights(model)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Add gradient norm monitoring\n",
    "        def hook_fn(grad):\n",
    "            if torch.isnan(grad).any():\n",
    "                logger.error(\"NaN detected in gradients!\")\n",
    "                return None\n",
    "            norm = grad.norm().item()\n",
    "            if norm > 10:\n",
    "                logger.warning(f\"Large gradient norm detected: {norm}\")\n",
    "            return grad\n",
    "        \n",
    "        # Attach hooks to monitor gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.register_hook(lambda grad, name=name: hook_fn(grad))\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, verbose=True)\n",
    "        recon_criterion = nn.MSELoss()\n",
    "        supervised_criterion = nn.MSELoss()\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=7)\n",
    "        best_val_loss = float('inf')\n",
    "        nan_epochs = 0  # Counter for epochs with NaN losses\n",
    "        \n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            # Training Phase with enhanced monitoring\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_steps = 0\n",
    "            epoch_grad_norms = []\n",
    "            \n",
    "            for file in files:\n",
    "                features, targets, weights = load_and_scale_data_for_dates(\n",
    "                    file, train_dates, feature_scaler, EXCLUDE_COLS\n",
    "                )\n",
    "                if features is None:\n",
    "                    continue\n",
    "                \n",
    "                # Check for NaN/Inf in input data\n",
    "                if np.isnan(features).any() or np.isinf(features).any():\n",
    "                    logger.warning(f\"NaN/Inf detected in features from {file}\")\n",
    "                    continue\n",
    "                    \n",
    "                train_dataset = FinancialDataset(features, targets, weights)\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    num_workers=4, pin_memory=True\n",
    "                )\n",
    "                \n",
    "                for batch_x, batch_y, batch_w in train_loader:\n",
    "                    batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    try:\n",
    "                        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                            reconstructed, mlp_output, _ = model(batch_x)\n",
    "                            loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                            loss_sup = supervised_criterion(mlp_output.squeeze(), batch_y)\n",
    "                            loss_sup = (loss_sup * batch_w).sum() / (batch_w.sum() + 1e-6)\n",
    "                            loss = loss_recon + loss_sup\n",
    "                            \n",
    "                        if torch.isnan(loss) or torch.isinf(loss):\n",
    "                            logger.error(f\"NaN/Inf loss detected in fold {fold_idx}, epoch {epoch}\")\n",
    "                            continue\n",
    "                            \n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        \n",
    "                        # Monitor gradient norms\n",
    "                        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        epoch_grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        \n",
    "                        train_loss += loss.item()\n",
    "                        train_steps += 1\n",
    "                        \n",
    "                    except RuntimeError as e:\n",
    "                        logger.error(f\"Runtime error in training: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_train_loss = train_loss / train_steps if train_steps > 0 else float('nan')\n",
    "            \n",
    "            # Log gradient statistics\n",
    "            if epoch_grad_norms:\n",
    "                logger.info(f\"Gradient norms - Mean: {np.mean(epoch_grad_norms):.4f}, \"\n",
    "                          f\"Max: {np.max(epoch_grad_norms):.4f}\")\n",
    "            \n",
    "            # Validation Phase with enhanced monitoring\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_steps = 0\n",
    "            all_targets, all_predictions = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for file in files:\n",
    "                    features, targets, weights = load_and_scale_data_for_dates(\n",
    "                        file, val_dates, feature_scaler, EXCLUDE_COLS\n",
    "                    )\n",
    "                    if features is None:\n",
    "                        continue\n",
    "                        \n",
    "                    val_dataset = FinancialDataset(features, targets, weights)\n",
    "                    val_loader = DataLoader(\n",
    "                        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True\n",
    "                    )\n",
    "                    \n",
    "                    for batch_x, batch_y, batch_w in val_loader:\n",
    "                        batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "                        \n",
    "                        try:\n",
    "                            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                                reconstructed, mlp_output, _ = model(batch_x)\n",
    "                                loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                                loss_sup = supervised_criterion(mlp_output.squeeze(), batch_y)\n",
    "                                loss_sup = (loss_sup * batch_w).sum() / (batch_w.sum() + 1e-6)\n",
    "                                loss = loss_recon + loss_sup\n",
    "                                \n",
    "                            val_loss += loss.item()\n",
    "                            val_steps += 1\n",
    "                            \n",
    "                            all_targets.extend(batch_y.cpu().numpy())\n",
    "                            all_predictions.extend(mlp_output.squeeze().cpu().numpy())\n",
    "                            \n",
    "                        except RuntimeError as e:\n",
    "                            logger.error(f\"Runtime error in validation: {str(e)}\")\n",
    "                            continue\n",
    "            \n",
    "            avg_val_loss = val_loss / val_steps if val_steps > 0 else float('nan')\n",
    "            \n",
    "            # Check for NaN losses\n",
    "            if np.isnan(avg_train_loss) or np.isnan(avg_val_loss):\n",
    "                nan_epochs += 1\n",
    "                logger.warning(f\"NaN loss detected in epoch {epoch}. NaN epoch count: {nan_epochs}\")\n",
    "                if nan_epochs >= 3:\n",
    "                    logger.error(f\"Too many NaN epochs in fold {fold_idx}. Stopping this fold.\")\n",
    "                    break\n",
    "            else:\n",
    "                nan_epochs = 0  # Reset counter if we get a valid epoch\n",
    "            \n",
    "            # R² calculation with enhanced error handling\n",
    "            try:\n",
    "                all_targets = np.array(all_targets)\n",
    "                all_predictions = np.array(all_predictions)\n",
    "                valid_mask = np.isfinite(all_targets) & np.isfinite(all_predictions)\n",
    "                filtered_targets = all_targets[valid_mask]\n",
    "                filtered_predictions = all_predictions[valid_mask]\n",
    "                r2 = r2_score(filtered_targets, filtered_predictions) if len(filtered_targets) > 1 else float('nan')\n",
    "                \n",
    "                logger.info(f\"[Fold {fold_idx} | Epoch {epoch}] \"\n",
    "                          f\"Train Loss: {avg_train_loss:.5f} | \"\n",
    "                          f\"Val Loss: {avg_val_loss:.5f} | \"\n",
    "                          f\"R²: {r2:.5f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating R² score: {str(e)}\")\n",
    "                r2 = float('nan')\n",
    "            \n",
    "            # Model saving with error handling\n",
    "            if avg_val_loss < best_val_loss and not np.isnan(avg_val_loss):\n",
    "                best_val_loss = avg_val_loss\n",
    "                try:\n",
    "                    torch.save(model.state_dict(), f\"best_model_fold_{fold_idx}.pt\")\n",
    "                    logger.info(f\"Best model updated for fold {fold_idx}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error saving model: {str(e)}\")\n",
    "            \n",
    "            # Scheduler and early stopping\n",
    "            scheduler.step(avg_val_loss)\n",
    "            early_stopping(avg_val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(f\"Early stopping triggered for Fold {fold_idx}\")\n",
    "                break\n",
    "\n",
    "# Use the enhanced training function\n",
    "enhanced_train_model_with_cv(folds, all_part_files, feature_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0d491-5076-445b-975c-0a91a3a6e4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#goat but does optuna which I dont have time for\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import json\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import amp\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#############################################\n",
    "# Setup Logging\n",
    "#############################################\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#############################################\n",
    "# 1) Reproducibility Setup\n",
    "#############################################\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "logger.info(\"Random seed set for reproducibility.\")\n",
    "\n",
    "#############################################\n",
    "# 2) Basic Setup\n",
    "#############################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "PARTITIONED_DIR = \"/home/jupyter/data/partitioned/\"\n",
    "all_part_files = sorted(glob.glob(os.path.join(PARTITIONED_DIR, \"XGFeatures_part_*.parquet\")))\n",
    "logger.info(f\"Found {len(all_part_files)} partitioned files.\")\n",
    "\n",
    "EXCLUDE_COLS = [\"date_id\", \"time_id\", \"symbol_id\", \"weight\", \"partition_id\"]\n",
    "\n",
    "# Directories for saving results\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "os.makedirs(\"saved_scalers\", exist_ok=True)\n",
    "\n",
    "#############################################\n",
    "# 3) Identify Unique Dates\n",
    "#############################################\n",
    "def get_unique_dates(part_files, num_files=None):\n",
    "    all_dates = set()\n",
    "    files_to_process = part_files if num_files is None else part_files[:num_files]\n",
    "    for f in files_to_process:\n",
    "        chunk_df = pl.read_parquet(f).select(pl.col(\"date_id\"))\n",
    "        unique_dates_chunk = chunk_df.unique().to_series().to_list()\n",
    "        all_dates.update(unique_dates_chunk)\n",
    "    sorted_dates = sorted(all_dates)\n",
    "    return sorted_dates\n",
    "\n",
    "sorted_dates = get_unique_dates(all_part_files, num_files=17)\n",
    "logger.info(f\"Identified {len(sorted_dates)} unique dates from first 17 chunks.\")\n",
    "\n",
    "#############################################\n",
    "# 4) Create \"Rolling\" Folds with a Gap\n",
    "#############################################\n",
    "def create_rolling_folds(dates_list, n_folds=5, gap_size=30, val_size=100, train_size=300):\n",
    "    folds = []\n",
    "    idx_start = 0\n",
    "    total_dates = len(dates_list)\n",
    "    required_per_fold = train_size + gap_size + val_size\n",
    "\n",
    "    max_possible_folds = (total_dates + gap_size) // required_per_fold\n",
    "    if max_possible_folds < n_folds:\n",
    "        logger.warning(f\"Requested {n_folds} folds, but only {max_possible_folds} can be created with {total_dates} dates.\")\n",
    "        n_folds = max_possible_folds\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        train_end = idx_start + train_size\n",
    "        if train_end >= total_dates:\n",
    "            logger.warning(f\"Not enough dates to create fold {i+1}.\")\n",
    "            break\n",
    "\n",
    "        gap_start = train_end\n",
    "        gap_end = gap_start + gap_size\n",
    "\n",
    "        val_start = gap_end\n",
    "        val_end = val_start + val_size\n",
    "\n",
    "        if val_end > total_dates:\n",
    "            val_end = total_dates\n",
    "\n",
    "        train_dates = dates_list[idx_start:train_end]\n",
    "        val_dates = dates_list[val_start:val_end]\n",
    "\n",
    "        # Update start index for next fold\n",
    "        idx_start = val_end\n",
    "\n",
    "        if not train_dates or not val_dates:\n",
    "            logger.warning(f\"Empty train or validation set for fold {i+1}. Skipping.\")\n",
    "            break\n",
    "\n",
    "        folds.append((train_dates, val_dates))\n",
    "        logger.info(f\"Created Fold {i+1}: Train={len(train_dates)} dates, Val={len(val_dates)} dates\")\n",
    "\n",
    "    logger.info(f\"Total folds created: {len(folds)}\")\n",
    "    return folds\n",
    "\n",
    "n_folds = 5\n",
    "folds = create_rolling_folds(sorted_dates, n_folds=n_folds, gap_size=30, val_size=100)\n",
    "logger.info(f\"Created {len(folds)} rolling folds with gap=30, val_size=100.\")\n",
    "for i, (td, vd) in enumerate(folds, start=1):\n",
    "    logger.info(f\"Fold {i} => Train: {len(td)} dates, Val: {len(vd)} dates\")\n",
    "\n",
    "#############################################\n",
    "# 5) Data Classes\n",
    "#############################################\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, features, target, weights):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx], self.weights[idx]\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.05):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.std > 0:\n",
    "            return x + torch.randn_like(x) * self.std\n",
    "        return x\n",
    "\n",
    "#############################################\n",
    "# 6) Model Definition\n",
    "#############################################\n",
    "class AutoencoderWithMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, dropout_rate=0.3, noise_std=0.05):\n",
    "        super().__init__()\n",
    "        self.noise = GaussianNoise(std=noise_std)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.bottleneck = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim + hidden_dim // 4, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_noised = self.noise(x)\n",
    "        encoded = self.encoder(x_noised)\n",
    "        latent = self.bottleneck(encoded)\n",
    "        reconstructed = self.decoder(latent)\n",
    "\n",
    "        mlp_input = torch.cat([x, latent], dim=1)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        return reconstructed, mlp_output, latent\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "#############################################\n",
    "# 7) Feature + Target Standardization\n",
    "#############################################\n",
    "def build_scalers_from_training_data(folds, chunk_files, exclude_cols, target_col=\"responder_6\"):\n",
    "    \"\"\"\n",
    "    Aggregates all training data across folds to fit the scalers.\n",
    "    Saves the scalers to disk.\n",
    "    Returns (feature_scaler, target_scaler).\n",
    "    \"\"\"\n",
    "    all_train_features = []\n",
    "    all_train_targets = []\n",
    "\n",
    "    logger.info(\"Aggregating training data across all folds to fit scalers.\")\n",
    "\n",
    "    for fold_i, (train_dates, _) in enumerate(folds, start=1):\n",
    "        logger.info(f\"Processing Fold {fold_i} for scaler fitting.\")\n",
    "        for cf in chunk_files:\n",
    "            # Load and filter training data for the current fold\n",
    "            df = pl.read_parquet(cf)\n",
    "            df_train = df.filter(pl.col(\"date_id\").is_in(list(train_dates)))\n",
    "            if df_train.is_empty():\n",
    "                continue\n",
    "\n",
    "            numeric_cols = [c for c in df_train.columns if (c not in exclude_cols) and (not c.startswith(\"responder_\"))]\n",
    "            features = df_train.select(numeric_cols).to_numpy().astype(np.float32)\n",
    "            all_train_features.append(features)\n",
    "\n",
    "            targets = df_train[target_col].to_numpy().astype(np.float32).reshape(-1, 1)\n",
    "            all_train_targets.append(targets)\n",
    "\n",
    "    if not all_train_features:\n",
    "        logger.error(\"No training data found to fit scalers.\")\n",
    "        raise ValueError(\"Training data is empty.\")\n",
    "\n",
    "    all_train_features = np.vstack(all_train_features)\n",
    "    all_train_targets = np.vstack(all_train_targets)\n",
    "\n",
    "    # Fit scalers\n",
    "    feature_scaler = StandardScaler()\n",
    "    feature_scaler.fit(all_train_features)\n",
    "\n",
    "    target_scaler = StandardScaler()\n",
    "    target_scaler.fit(all_train_targets)\n",
    "\n",
    "    # Save scalers\n",
    "    with open(\"saved_scalers/feature_scaler.pkl\", \"wb\") as f:\n",
    "        import pickle\n",
    "        pickle.dump(feature_scaler, f)\n",
    "    with open(\"saved_scalers/target_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(target_scaler, f)\n",
    "\n",
    "    logger.info(\"Feature and target scalers built and saved from aggregated training data.\")\n",
    "    return feature_scaler, target_scaler\n",
    "\n",
    "#############################################\n",
    "# 8) Load & Filter by Date with Scaling\n",
    "#############################################\n",
    "def load_and_filter_chunk(\n",
    "    parquet_file,\n",
    "    date_set,\n",
    "    exclude_cols,\n",
    "    feat_scaler,\n",
    "    targ_scaler,\n",
    "    target_col=\"responder_6\",\n",
    "    weight_col=\"weight\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load chunk\n",
    "    2) Filter rows by date_id in date_set\n",
    "    3) Scale features + target\n",
    "    4) Return (features, target, weights)\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(parquet_file)\n",
    "    df = df.filter(pl.col(\"date_id\").is_in(list(date_set)))\n",
    "    if df.is_empty():\n",
    "        return None, None, None\n",
    "\n",
    "    # Features\n",
    "    numeric_cols = [c for c in df.columns if (c not in exclude_cols) and (not c.startswith(\"responder_\"))]\n",
    "    f = df.select(numeric_cols).to_numpy().astype(np.float32)\n",
    "    f = feat_scaler.transform(f)\n",
    "\n",
    "    # Target\n",
    "    t = df[target_col].to_numpy().astype(np.float32).reshape(-1, 1)\n",
    "    t = targ_scaler.transform(t).flatten()\n",
    "\n",
    "    # Weights\n",
    "    w = df[weight_col].to_numpy().astype(np.float32)\n",
    "\n",
    "    return f, t, w\n",
    "\n",
    "#############################################\n",
    "# 9) Early Stopping Class\n",
    "#############################################\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "#############################################\n",
    "# 10) Training Function\n",
    "#############################################\n",
    "def train_one_fold(\n",
    "    model,\n",
    "    train_date_set,\n",
    "    val_date_set,\n",
    "    chunk_files,\n",
    "    exclude_cols,\n",
    "    feat_scaler,\n",
    "    targ_scaler,\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    batch_size=512,\n",
    "    patience=5,\n",
    "    device=device,\n",
    "    fold_i=1  # Added to save models per fold\n",
    "):\n",
    "    \"\"\"\n",
    "    Each epoch:\n",
    "        - Iterate over all chunk files and train on rows matching train_date_set.\n",
    "        - Validate on the validation set at the end of the epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Loss functions\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    supervised_criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Prepare validation data\n",
    "    val_feats_list, val_targs_list, val_wts_list = [], [], []\n",
    "    for cf in chunk_files:\n",
    "        vf, vt, vw = load_and_filter_chunk(cf, val_date_set, exclude_cols, feat_scaler, targ_scaler)\n",
    "        if vf is not None and len(vf) > 0:\n",
    "            val_feats_list.append(vf)\n",
    "            val_targs_list.append(vt)\n",
    "            val_wts_list.append(vw)\n",
    "\n",
    "    if val_feats_list:\n",
    "        val_feats = np.concatenate(val_feats_list, axis=0)\n",
    "        val_targs = np.concatenate(val_targs_list, axis=0)\n",
    "        val_wts = np.concatenate(val_wts_list, axis=0)\n",
    "        val_ds = FinancialDataset(val_feats, val_targs, val_wts)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    else:\n",
    "        logger.warning(\"No validation data found for this fold.\")\n",
    "        val_loader = None\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        train_steps = 0\n",
    "\n",
    "        for cf in chunk_files:\n",
    "            feats, targs, wts = load_and_filter_chunk(cf, train_date_set, exclude_cols, feat_scaler, targ_scaler)\n",
    "            if feats is None or len(feats) == 0:\n",
    "                continue\n",
    "\n",
    "            train_ds = FinancialDataset(feats, targs, wts)\n",
    "            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "            for batch_x, batch_y, batch_w in train_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_w = batch_w.to(device)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with amp.autocast(device_type=device.type):\n",
    "                    reconstructed, mlp_out, _ = model(batch_x)\n",
    "                    loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                    loss_sup = supervised_criterion(mlp_out.squeeze(), batch_y)\n",
    "                    loss_sup = (loss_sup * batch_w).mean()\n",
    "                    loss = loss_recon + loss_sup\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss_sum += loss.item()\n",
    "                train_steps += 1\n",
    "\n",
    "        train_loss_avg = train_loss_sum / max(1, train_steps)\n",
    "\n",
    "        # Validation Pass\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_steps = 0\n",
    "        weighted_sse = 0.0\n",
    "        weighted_sst = 0.0\n",
    "\n",
    "        if val_loader:\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y, batch_w in val_loader:\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    batch_y = batch_y.to(device)\n",
    "                    batch_w = batch_w.to(device)\n",
    "\n",
    "                    with amp.autocast(device_type=device.type):\n",
    "                        reconstructed, mlp_out, _ = model(batch_x)\n",
    "                        loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                        loss_sup = supervised_criterion(mlp_out.squeeze(), batch_y)\n",
    "                        loss_sup = (loss_sup * batch_w).mean()\n",
    "                        loss = loss_recon + loss_sup\n",
    "\n",
    "                    val_loss_sum += loss.item()\n",
    "                    val_steps += 1\n",
    "\n",
    "                    # Compute R² components\n",
    "                    weighted_sse += (batch_w * (batch_y - mlp_out.squeeze()) ** 2).sum().item()\n",
    "                    weighted_sst += (batch_w * batch_y ** 2).sum().item()\n",
    "\n",
    "        if val_steps > 0:\n",
    "            val_loss_avg = val_loss_sum / val_steps\n",
    "            r2 = 1 - (weighted_sse / weighted_sst) if weighted_sst != 0 else float('nan')\n",
    "        else:\n",
    "            val_loss_avg = float(\"inf\")\n",
    "            r2 = float('nan')\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss_avg)\n",
    "\n",
    "        logger.info(f\"[Fold {fold_i} Epoch {epoch}/{epochs}] Train Loss: {train_loss_avg:.5f} | Val Loss: {val_loss_avg:.5f} | R²: {r2:.5f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        early_stopping(val_loss_avg)\n",
    "        if early_stopping.early_stop:\n",
    "            logger.info(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Save Best Model for this Fold\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            model_path = f\"saved_models/best_fold_{fold_i}_model.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            logger.info(f\"Best model for Fold {fold_i} saved to {model_path}.\")\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "#############################################\n",
    "# 11) Optuna Objective Function\n",
    "#############################################\n",
    "def objective_rolling_tscv(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 256, 1024, step=256)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    noise_std = trial.suggest_float(\"noise_std\", 0.0, 0.1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    epochs = 3  # Reduced epochs for faster trials\n",
    "\n",
    "    fold_losses = []\n",
    "    for fold_i, (train_dates, val_dates) in enumerate(folds, start=1):\n",
    "        logger.info(f\"Starting Fold {fold_i} with {len(train_dates)} training dates and {len(val_dates)} validation dates.\")\n",
    "        model = AutoencoderWithMLP(\n",
    "            input_dim=GLOBAL_INPUT_DIM,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "            noise_std=noise_std\n",
    "        )\n",
    "        initialize_weights(model)\n",
    "\n",
    "        val_loss = train_one_fold(\n",
    "            model=model,\n",
    "            train_date_set=set(train_dates),\n",
    "            val_date_set=set(val_dates),\n",
    "            chunk_files=all_part_files,\n",
    "            exclude_cols=EXCLUDE_COLS,\n",
    "            feat_scaler=feature_scaler,\n",
    "            targ_scaler=target_scaler,\n",
    "            epochs=epochs,\n",
    "            lr=learning_rate,\n",
    "            batch_size=512,\n",
    "            patience=5,\n",
    "            device=device,\n",
    "            fold_i=fold_i  # Pass fold index\n",
    "        )\n",
    "        fold_losses.append(val_loss)\n",
    "        logger.info(f\"Fold {fold_i} completed with Val Loss: {val_loss:.5f}\")\n",
    "\n",
    "    avg_val_loss = np.mean(fold_losses)\n",
    "    logger.info(f\"Average Validation Loss for Trial: {avg_val_loss:.5f}\")\n",
    "    return avg_val_loss\n",
    "\n",
    "#############################################\n",
    "# 12) Evaluation on Separate Test Set\n",
    "#############################################\n",
    "def evaluate_on_test_set(best_model_paths, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the best models from each fold on the separate test set.\n",
    "    Aggregates the results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for fold_i, model_path in enumerate(best_model_paths, start=1):\n",
    "        logger.info(f\"Loading Best Model for Fold {fold_i} from {model_path}.\")\n",
    "        model = AutoencoderWithMLP(input_dim=GLOBAL_INPUT_DIM)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        val_steps = 0\n",
    "        weighted_sse = 0.0\n",
    "        weighted_sst = 0.0\n",
    "\n",
    "        all_preds = []\n",
    "        all_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_w in test_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_w = batch_w.to(device)\n",
    "\n",
    "                with amp.autocast(device_type=device.type):\n",
    "                    reconstructed, mlp_out, _ = model(batch_x)\n",
    "                    loss_recon = nn.MSELoss()(reconstructed, batch_x)\n",
    "                    loss_sup = nn.MSELoss()(mlp_out.squeeze(), batch_y)\n",
    "                    loss_sup = (loss_sup * batch_w).mean()\n",
    "                    loss = loss_recon + loss_sup\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                val_steps += 1\n",
    "\n",
    "                # Compute R² components\n",
    "                weighted_sse += (batch_w * (batch_y - mlp_out.squeeze()) ** 2).sum().item()\n",
    "                weighted_sst += (batch_w * batch_y ** 2).sum().item()\n",
    "\n",
    "                all_preds.extend(mlp_out.squeeze().cpu().numpy())\n",
    "                all_trues.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        if val_steps > 0:\n",
    "            avg_loss = total_loss / val_steps\n",
    "            r2 = 1 - (weighted_sse / weighted_sst) if weighted_sst != 0 else float('nan')\n",
    "        else:\n",
    "            avg_loss = float(\"inf\")\n",
    "            r2 = float('nan')\n",
    "\n",
    "        results.append({\n",
    "            \"fold\": fold_i,\n",
    "            \"model_path\": model_path,\n",
    "            \"test_loss\": avg_loss,\n",
    "            \"r2_score\": r2\n",
    "        })\n",
    "\n",
    "        logger.info(f\"[Test Evaluation] Fold {fold_i} | Test Loss: {avg_loss:.5f} | R²: {r2:.5f}\")\n",
    "\n",
    "    # Aggregate Results\n",
    "    avg_test_loss = np.mean([res[\"test_loss\"] for res in results])\n",
    "    avg_r2 = np.mean([res[\"r2_score\"] for res in results if not np.isnan(res[\"r2_score\"])])\n",
    "\n",
    "    logger.info(f\"Average Test Loss across all folds: {avg_test_loss:.5f}\")\n",
    "    logger.info(f\"Average Test R² across all folds: {avg_r2:.5f}\")\n",
    "\n",
    "    # Save Test Results\n",
    "    with open(\"saved_models/test_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    with open(\"saved_models/aggregate_test_evaluation.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"average_test_loss\": avg_test_loss,\n",
    "            \"average_test_r2\": avg_r2\n",
    "        }, f)\n",
    "\n",
    "    logger.info(\"Test evaluation results saved.\")\n",
    "\n",
    "#############################################\n",
    "# 13) Main Execution\n",
    "#############################################\n",
    "if __name__ == \"__main__\":\n",
    "    #########################################\n",
    "    # (A) Build Scalers from All Training Data\n",
    "    #########################################\n",
    "    feature_scaler, target_scaler = build_scalers_from_training_data(folds, all_part_files, EXCLUDE_COLS, target_col=\"responder_6\")\n",
    "\n",
    "    #########################################\n",
    "    # (B) Determine Input Dimension\n",
    "    #########################################\n",
    "    # Assuming all chunks have the same structure\n",
    "    df_tmp = pl.read_parquet(all_part_files[0])\n",
    "    numeric_cols = [c for c in df_tmp.columns if (c not in EXCLUDE_COLS) and (not c.startswith(\"responder_\"))]\n",
    "    GLOBAL_INPUT_DIM = len(numeric_cols)\n",
    "    logger.info(f\"GLOBAL_INPUT_DIM set to {GLOBAL_INPUT_DIM}.\")\n",
    "    del df_tmp\n",
    "\n",
    "    #########################################\n",
    "    # (C) Optuna Hyperparameter Tuning\n",
    "    #########################################\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective_rolling_tscv, n_trials=100, n_jobs=1)  # Set n_jobs=1 to avoid potential issues with DataLoader\n",
    "    logger.info(\"Optuna hyperparameter tuning completed.\")\n",
    "    logger.info(f\"Best Trial: {study.best_trial.value}\")\n",
    "    logger.info(f\"Best Params: {study.best_params}\")\n",
    "\n",
    "    # Save Best Hyperparameters\n",
    "    with open(\"saved_models/best_hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(study.best_params, f)\n",
    "    logger.info(\"Best hyperparameters saved to 'saved_models/best_hyperparameters.json'.\")\n",
    "\n",
    "    #########################################\n",
    "    # (D) Final Training with Best Hyperparameters\n",
    "    #########################################\n",
    "    best_params = study.best_params\n",
    "    final_fold_losses = []\n",
    "    final_epochs = 75  # Adjust based on available resources\n",
    "\n",
    "    best_model_paths = []  # To store paths of best models per fold\n",
    "\n",
    "    for fold_i, (train_dates, val_dates) in enumerate(folds, start=1):\n",
    "        logger.info(f\"Starting Final Training for Fold {fold_i}.\")\n",
    "\n",
    "        final_model = AutoencoderWithMLP(\n",
    "            input_dim=GLOBAL_INPUT_DIM,\n",
    "            hidden_dim=best_params[\"hidden_dim\"],\n",
    "            dropout_rate=best_params[\"dropout_rate\"],\n",
    "            noise_std=best_params[\"noise_std\"]\n",
    "        )\n",
    "        initialize_weights(final_model)\n",
    "\n",
    "        val_loss = train_one_fold(\n",
    "            model=final_model,\n",
    "            train_date_set=set(train_dates),\n",
    "            val_date_set=set(val_dates),\n",
    "            chunk_files=all_part_files,\n",
    "            exclude_cols=EXCLUDE_COLS,\n",
    "            feat_scaler=feature_scaler,\n",
    "            targ_scaler=target_scaler,\n",
    "            epochs=final_epochs,\n",
    "            lr=best_params[\"learning_rate\"],\n",
    "            batch_size=512,\n",
    "            patience=5,\n",
    "            device=device,\n",
    "            fold_i=fold_i\n",
    "        )\n",
    "        final_fold_losses.append(val_loss)\n",
    "        logger.info(f\"[FINAL Fold {fold_i}] Validation Loss: {val_loss:.5f}\")\n",
    "\n",
    "        # Save Final Model for Each Fold\n",
    "        final_model_path = f\"saved_models/final_fold_{fold_i}_model.pt\"\n",
    "        torch.save(final_model.state_dict(), final_model_path)\n",
    "        logger.info(f\"Final model for Fold {fold_i} saved to {final_model_path}.\")\n",
    "        best_model_paths.append(final_model_path)\n",
    "\n",
    "    overall_loss = np.mean(final_fold_losses)\n",
    "    logger.info(f\"Final Average Validation Loss over {len(folds)} folds: {overall_loss:.5f}\")\n",
    "\n",
    "    # Save Overall Metrics\n",
    "    with open(\"saved_models/final_validation_loss.json\", \"w\") as f:\n",
    "        json.dump({\"average_val_loss\": overall_loss}, f)\n",
    "    logger.info(\"Final validation loss metrics saved.\")\n",
    "\n",
    "    #########################################\n",
    "    # (E) Evaluation on Separate Test Set\n",
    "    #########################################\n",
    "    test_file = \"/home/jupyter/data/jane-street/test.parquet\"\n",
    "    logger.info(f\"Loading separate test set from {test_file}.\")\n",
    "\n",
    "    # Load the test set\n",
    "    test_df = pl.read_parquet(test_file)\n",
    "\n",
    "    # Preprocess the test set\n",
    "    # Assuming the test set has the same structure as the training/validation sets\n",
    "    test_dates = test_df[\"date_id\"].unique().to_list()\n",
    "    logger.info(f\"Test set contains {len(test_dates)} unique dates.\")\n",
    "\n",
    "    # Since the test set is separate, no need to split further\n",
    "    test_feats, test_targs, test_wts = load_and_filter_chunk(\n",
    "        parquet_file=test_file,\n",
    "        date_set=set(test_dates),\n",
    "        exclude_cols=EXCLUDE_COLS,\n",
    "        feat_scaler=feature_scaler,\n",
    "        targ_scaler=target_scaler,\n",
    "        target_col=\"responder_6\",\n",
    "        weight_col=\"weight\"\n",
    "    )\n",
    "\n",
    "    if test_feats is None or len(test_feats) == 0:\n",
    "        logger.error(\"Test set is empty after filtering. Aborting test evaluation.\")\n",
    "    else:\n",
    "        # Create Test Dataset and DataLoader\n",
    "        test_dataset = FinancialDataset(test_feats, test_targs, test_wts)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        # Evaluate each best model on the test set\n",
    "        evaluate_on_test_set(best_model_paths, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998d843-728f-4ce9-a4a1-cf340a9faa57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#did not evaluate\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import amp\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import logging\n",
    "\n",
    "#############################################\n",
    "# Setup Logging\n",
    "#############################################\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#############################################\n",
    "# 1) Reproducibility Setup\n",
    "#############################################\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "#############################################\n",
    "# 2) Paths and Constants\n",
    "#############################################\n",
    "PARTITIONED_DIR = \"/home/jupyter/data/partitioned/\"\n",
    "all_part_files = sorted(glob.glob(os.path.join(PARTITIONED_DIR, \"*.parquet\")))\n",
    "logger.info(f\"Found {len(all_part_files)} partitioned files.\")\n",
    "\n",
    "# Load column names to exclude all responders dynamically\n",
    "EXCLUDE_COLS = [\"date_id\", \"time_id\", \"symbol_id\", \"weight\", \"partition_id\"]\n",
    "df_tmp = pl.read_parquet(all_part_files[0])\n",
    "EXCLUDE_COLS += [col for col in df_tmp.columns if col.startswith(\"responder_\")]\n",
    "del df_tmp\n",
    "\n",
    "# Best hyperparameters (from the second script)\n",
    "HIDDEN_DIM = 512\n",
    "DROPOUT_RATE = 0.2537\n",
    "NOISE_STD = 0.0017\n",
    "LEARNING_RATE = 4.16276e-05\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 75\n",
    "\n",
    "#############################################\n",
    "# 3) Data Classes\n",
    "#############################################\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, features, target, weights):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx], self.weights[idx]\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.05):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.std > 0:\n",
    "            return x + torch.randn_like(x) * self.std\n",
    "        return x\n",
    "\n",
    "class AutoencoderWithMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate, noise_std):\n",
    "        super().__init__()\n",
    "        self.noise = GaussianNoise(std=noise_std)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.bottleneck = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        # MLP for Final Prediction\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim + hidden_dim // 4, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_noised = self.noise(x)\n",
    "        encoded = self.encoder(x_noised)\n",
    "        latent = self.bottleneck(encoded)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        mlp_input = torch.cat([x, latent], dim=1)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        return reconstructed, mlp_output, latent\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "#############################################\n",
    "# 4) Standardization & Scalers\n",
    "#############################################\n",
    "def build_scalers(chunk_files, exclude_cols):\n",
    "    all_features = []\n",
    "    for file in chunk_files:\n",
    "        df = pl.read_parquet(file)\n",
    "        numeric_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        features = df.select(numeric_cols).to_numpy()\n",
    "        all_features.append(features)\n",
    "\n",
    "    all_features = np.vstack(all_features)\n",
    "    feature_scaler = StandardScaler()\n",
    "    feature_scaler.fit(all_features)\n",
    "\n",
    "    # Save scaler\n",
    "    with open(\"saved_scalers/feature_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(feature_scaler, f)\n",
    "\n",
    "    return feature_scaler\n",
    "\n",
    "# Build feature scaler\n",
    "feature_scaler = build_scalers(all_part_files, EXCLUDE_COLS)\n",
    "GLOBAL_INPUT_DIM = len([col for col in pl.read_parquet(all_part_files[0]).columns if col not in EXCLUDE_COLS])\n",
    "\n",
    "#############################################\n",
    "# 5) Model Training Function\n",
    "#############################################\n",
    "def train_model():\n",
    "    model = AutoencoderWithMLP(\n",
    "        input_dim=GLOBAL_INPUT_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        noise_std=NOISE_STD\n",
    "    )\n",
    "    initialize_weights(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    supervised_criterion = nn.MSELoss()\n",
    "    scaler_amp = amp.GradScaler()\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for cf in all_part_files:\n",
    "            df = pl.read_parquet(cf)\n",
    "            numeric_cols = [col for col in df.columns if col not in EXCLUDE_COLS]\n",
    "            features = feature_scaler.transform(df.select(numeric_cols).to_numpy())\n",
    "            targets = df[\"responder_6\"].to_numpy()\n",
    "            weights = df[\"weight\"].to_numpy()\n",
    "\n",
    "            dataset = FinancialDataset(features, targets, weights)\n",
    "            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "            for batch_x, batch_y, batch_w in loader:\n",
    "                batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with amp.autocast(device_type=device.type):\n",
    "                    reconstructed, mlp_output, _ = model(batch_x)\n",
    "                    loss_recon = recon_criterion(reconstructed, batch_x)\n",
    "                    loss_sup = (supervised_criterion(mlp_output.squeeze(), batch_y) * batch_w).mean()\n",
    "                    loss = loss_recon + loss_sup\n",
    "\n",
    "                scaler_amp.scale(loss).backward()\n",
    "                scaler_amp.step(optimizer)\n",
    "                scaler_amp.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(all_part_files)\n",
    "        scheduler.step(avg_loss)\n",
    "        logger.info(f\"Epoch {epoch} | Avg Loss: {avg_loss:.5f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"saved_models/final_model.pt\")\n",
    "    logger.info(\"Model training completed and saved!\")\n",
    "\n",
    "#############################################\n",
    "# 6) Execute Training\n",
    "#############################################\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732c1ed-8105-41d3-8bcc-ba0b5cb10006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
