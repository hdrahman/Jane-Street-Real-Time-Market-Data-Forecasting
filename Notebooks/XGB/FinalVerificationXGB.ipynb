{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8802b0c-f985-4386-b289-f64f783390f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_parquet_file(input_path, output_path):\n",
    "    df = pl.read_parquet(input_path)\n",
    "    df = df.with_columns([\n",
    "        pl.col(col).cast(pl.Float32)\n",
    "        for col, dtype in df.schema.items()\n",
    "        if dtype == pl.Float64\n",
    "    ])\n",
    "    df.write_parquet(output_path)\n",
    "\n",
    "def process_partitioned_data(input_dir, output_dir, num_workers=4):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    parquet_files = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(input_dir)\n",
    "        for file in files if file.endswith(\".parquet\")\n",
    "    ]\n",
    "\n",
    "    output_files = [\n",
    "        os.path.join(output_dir, os.path.relpath(file, input_dir))\n",
    "        for file in parquet_files\n",
    "    ]\n",
    "    \n",
    "    for output_file in output_files:\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        executor.map(process_parquet_file, parquet_files, output_files)\n",
    "\n",
    "input_directory = \"/home/jupyter/data/XGFeatures_partitioned\"\n",
    "output_directory = \"/home/jupyter/data/XGFeatures_partitioned\"\n",
    "process_partitioned_data(input_directory, output_directory, num_workers=8)#change number of workers accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34fdf5-264d-4683-92c1-d742e8ef4762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "data = pl.read_parquet(\"/home/jupyter/data/XGFeatures_partitioned/\", use_pyarrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bcd03-9dc6-4eba-a16c-f1366b76c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.with_columns([\n",
    "    pl.col(col).cast(pl.Float32) \n",
    "    for col, dtype in data.schema.items() \n",
    "    if dtype == pl.Float64\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b86313-d6ec-4cdc-ab47-01e9c89ad1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"../data/XGFeatures_row_partitioned/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "chunk_size = 2_000_000\n",
    "\n",
    "for i in range(0, len(data), chunk_size):\n",
    "    chunk = data.slice(i, chunk_size)\n",
    "    chunk.write_parquet(\n",
    "        f\"{output_dir}/XGFeatures_part_{i // chunk_size}.parquet\",\n",
    "        compression=\"zstd\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2a990-9187-421b-b416-64230eff9381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#verify no null\n",
    "null= data.null_count()\n",
    "total=0\n",
    "for col in null:\n",
    "    total += col[0]\n",
    "\n",
    "print(f'total null: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f06db-d6b5-4f80-b6b8-b6894226df16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check responder_6 distribution\n",
    "print(data.select(pl.col('responder_6')).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b941366-7d84-41ed-bd66-9c6f193fc9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert Polars DataFrame to Pandas for visualization\n",
    "responder_6_data = data.select(pl.col(\"responder_6\")).to_pandas()\n",
    "\n",
    "# Plot the histogram with Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(responder_6_data[\"responder_6\"], bins=50, kde=True, color=\"blue\")\n",
    "plt.title(\"Distribution of responder_6\", fontsize=16)\n",
    "plt.xlabel(\"Responder 6\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfc49a-2c64-4e9e-8f08-fb032a17a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_columns = [col for col in data.columns if data.select(pl.col(col).n_unique())[0, 0] == 1]\n",
    "print(f\"Constant columns: {constant_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c4b33-d1f8-43d6-bb19-20c6bf7ce2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics to check for extreme outliers\n",
    "summary_stats = data.select(pl.col(\"*\")).describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bba43-b185-4bea-88fe-6972a129dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns of low correlation with f6 ignore for now, seems too tight \n",
    "\n",
    "numerical_columns = [\n",
    "    col for col in data.columns if col not in ['date_id', 'time_id', 'symbol_id', 'weight', 'partition_id']\n",
    "    and not col.startswith('responder_')  # Exclude all responder columns\n",
    "]\n",
    "\n",
    "correlation_with_target = {}\n",
    "for col in numerical_columns:\n",
    "    correlation = data.select(\n",
    "        pl.corr(pl.col(col), pl.col(\"responder_6\")).alias(col)\n",
    "    ).to_dict(as_series=False)\n",
    "    correlation_with_target[col] = correlation[col][0]\n",
    "\n",
    "low_corr_threshold = 0.05\n",
    "low_corr_features = [col for col, corr in correlation_with_target.items() if abs(corr) < low_corr_threshold]\n",
    "\n",
    "print(f\"Low-correlation features: {low_corr_features}\")\n",
    "if low_corr_features:\n",
    "    data = data.drop(columns=low_corr_features)\n",
    "    print(f\"Dropped {len(low_corr_features)} features with low correlation to responder_6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0213c-00d5-4415-b218-b816814883fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 split\n",
    "split_date = data.select(pl.col(\"date_id\").quantile(0.8)).to_numpy().item()\n",
    "\n",
    "train_data = data.filter(pl.col(\"date_id\") < split_date)\n",
    "valid_data = data.filter(pl.col(\"date_id\") >= split_date)\n",
    "\n",
    "print(f\"Training set: {len(train_data)} rows\")\n",
    "print(f\"Validation set: {len(valid_data)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f4710-f603-4d46-a8f4-f75294c06919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "import xgboost as xgb\n",
    "\n",
    "features = [col for col in train_data.columns if col not in [\"responder_6\", \"date_id\", \"time_id\"]]\n",
    "\n",
    "dtrain = xgb.DMatrix(\n",
    "    data=train_data.select(features).to_pandas(),\n",
    "    label=train_data[\"responder_6\"].to_pandas()\n",
    ")\n",
    "\n",
    "model = xgb.train(\n",
    "    params={'tree_method': 'hist', 'max_depth': 3, 'n_estimators': 50},\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=100\n",
    ")\n",
    "\n",
    "importance = model.get_score(importance_type='weight')\n",
    "\n",
    "important_features = sorted(importance.items(), key=lambda x: -x[1])\n",
    "print(\"Top important features:\", important_features[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922754d9-710b-4ba5-924c-cfb8f7819838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#more efficient feature importance test\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Sample a subset of the data (70%)\n",
    "sample_size = int(len(train_data) * 0.5)\n",
    "subset = train_data.sample(n=sample_size)\n",
    "\n",
    "features = [col for col in train_data.columns if col not in [\"responder_6\", \"date_id\", \"time_id\"]]\n",
    "X_subset = subset.select(features).to_pandas()\n",
    "y_subset = subset[\"responder_6\"].to_pandas()\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_subset, label=y_subset)\n",
    "\n",
    "model = xgb.train(\n",
    "    params={'tree_method': 'hist', 'max_depth': 3, 'n_estimators': 50},\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=50\n",
    ")\n",
    "\n",
    "importance = model.get_score(importance_type='weight')\n",
    "important_features = sorted(importance.items(), key=lambda x: -x[1])\n",
    "print(\"Top important features:\", important_features[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc78d4-5207-4c4a-9543-9865f329c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no leakage in target variables\n",
    "# Avoid lagged values of responder_6 and ensure no overlap with prediction window\n",
    "leakage_check = [col for col in data.columns if \"responder\" in col and \"lag\" in col]\n",
    "print(\"Potential leakage columns:\", leakage_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642cd70-c76a-42fd-9cf7-6f3ade9b06d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data[\"responder_1_lag_1\"] == data[\"responder_1\"].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a6116-1ed7-4554-adab-6c37786a2371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check alignment of responder_1_lag_1\n",
    "data_check0 = data.select([\n",
    "    \"date_id\",\n",
    "    \"responder_1\",\n",
    "    pl.col(\"responder_1\").shift(1).alias(\"responder_1_lag_1\")\n",
    "])\n",
    "\n",
    "data_check1 = data.select([\n",
    "    \"date_id\",\n",
    "    \"responder_2\",\n",
    "    pl.col(\"responder_2\").shift(1).alias(\"responder_2_lag_1\")\n",
    "])\n",
    "\n",
    "print(data_check.head(10))\n",
    "print(data_check1.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7a77c-8e06-4f9c-9513-f457245f2253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing `date_id` or `time_id` combinations\n",
    "missing_rows = data.unique(subset=[\"date_id\", \"time_id\"]).filter(pl.col(\"date_id\").diff() > 1)\n",
    "print(missing_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d70a4-a419-443b-8547-3fcf0001551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
