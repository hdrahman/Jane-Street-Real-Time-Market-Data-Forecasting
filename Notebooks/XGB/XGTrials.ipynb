{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33eb7cd-72ef-49cd-8f48-108b8401aceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "partitioned_dir = \"/home/jupyter/data/XGFeatures_partitioned/\"\n",
    "data = pl.read_parquet(partitioned_dir, use_pyarrow=True)\n",
    "\n",
    "#Competition scoring\n",
    "def weighted_r2_score(y_true, y_pred, weights):\n",
    "    numerator = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "    denominator = np.sum(weights * y_true ** 2)\n",
    "    r2 = 1 - (numerator / denominator)\n",
    "    return r2\n",
    "\n",
    "# Use percentage of data(20%)\n",
    "sample_fraction = 0.35\n",
    "data = data.sample(fraction=sample_fraction, seed=42)\n",
    "\n",
    "# 80-20 split\n",
    "split_date = data.select(pl.col(\"date_id\").quantile(0.8)).to_numpy().item()\n",
    "train_data = data.filter(pl.col(\"date_id\") < split_date)\n",
    "valid_data = data.filter(pl.col(\"date_id\") >= split_date)\n",
    "\n",
    "# Convert training data to DMatrix\n",
    "X_train = train_data.select(pl.exclude([\"responder_6\", \"date_id\", \"time_id\", \"weight\"])).to_pandas()\n",
    "y_train = train_data[\"responder_6\"].to_pandas()\n",
    "train_weights = train_data[\"weight\"].to_pandas()\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, weight=train_weights)\n",
    "\n",
    "# Convert validation data to DMatrix\n",
    "X_valid = valid_data.select(pl.exclude([\"responder_6\", \"date_id\", \"time_id\", \"weight\"])).to_pandas()\n",
    "y_valid = valid_data[\"responder_6\"].to_pandas()\n",
    "valid_weights = valid_data[\"weight\"].to_pandas()\n",
    "\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid, weight=valid_weights)\n",
    "\n",
    "\n",
    "# Training --> base\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29df36-982d-44e6-a46f-495924a7000f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_04', 'feature_06', 'feature_07', 'responder_0', 'responder_1',\n",
      "       'responder_2', 'responder_3', 'responder_4', 'responder_5',\n",
      "       'responder_7', 'responder_8', 'responder_0_lag_1', 'responder_1_lag_1',\n",
      "       'responder_2_lag_1', 'responder_3_lag_1', 'responder_4_lag_1',\n",
      "       'responder_5_lag_1', 'responder_7_lag_1', 'responder_8_lag_1',\n",
      "       'responder_3_lag_diff_1', 'responder_5_lag_diff_1',\n",
      "       'responder_7_lag_diff_1', 'responder_8_lag_diff_1'],\n",
      "      dtype='object')\n",
      "[0]\ttrain-rmse:0.79647\tvalid-rmse:0.70635\n",
      "[1]\ttrain-rmse:0.73464\tvalid-rmse:0.65294\n",
      "[2]\ttrain-rmse:0.68022\tvalid-rmse:0.60692\n",
      "[3]\ttrain-rmse:0.63231\tvalid-rmse:0.56718\n",
      "[4]\ttrain-rmse:0.59038\tvalid-rmse:0.53353\n",
      "[5]\ttrain-rmse:0.55324\tvalid-rmse:0.50426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Evaluate models with top 10, 15, and 20 features\n",
    "k_features = [23] #23 is top\n",
    "\n",
    "for k in k_features:\n",
    "    # Select top k features using univariate feature selection\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    selector.fit(X_train_np, y_train_np)\n",
    "    \n",
    "    # Get the selected top features\n",
    "    top_feature_indices = selector.get_support(indices=True)\n",
    "    top_features = X_train.columns[top_feature_indices]\n",
    "    print(top_features)\n",
    "    \n",
    "    # Train and evaluate the model with top features\n",
    "    dtrain_top = xgb.DMatrix(X_train[top_features].to_numpy(), label=y_train, weight=train_weights)\n",
    "    dvalid_top = xgb.DMatrix(X_valid[top_features].to_numpy(), label=y_valid, weight=valid_weights)\n",
    "    \n",
    "    model_top = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_top,\n",
    "        num_boost_round=100,\n",
    "        evals=[(dtrain_top, \"train\"), (dvalid_top, \"valid\")],\n",
    "        early_stopping_rounds=10,\n",
    "    )\n",
    "    \n",
    "    y_pred_top = model_top.predict(dvalid_top)\n",
    "    r2_score_top = weighted_r2_score(y_valid.to_numpy(), y_pred_top, valid_weights.to_numpy())\n",
    "    print(f\"Top {k} Features Model Weighted R² Score: {r2_score_top:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354e58a-8a9d-4189-8d57-5228c6708b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = xgb.train( #-->no longer needed\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "    early_stopping_rounds=10,\n",
    ")\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(dvalid)\n",
    "\n",
    "# Calculate weighted R² score\n",
    "r2_score = weighted_r2_score(y_valid.to_numpy(), y_pred, valid_weights.to_numpy())\n",
    "#print(f\"Weighted R² Score: {r2_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce163be-39cf-4aee-98b5-9d177920ee60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate correlations of features with the target variable\n",
    "\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "correlations = np.abs(np.corrcoef(X_train_np.T, y_train_np, rowvar=True)[:-1, -1])\n",
    "correlation_threshold = 0.3\n",
    "\n",
    "# Filter out constant columns from X_train\n",
    "non_constant_columns = X_train.columns[X_train.std(axis=0) > 0]\n",
    "X_train_filtered = X_train[non_constant_columns]\n",
    "\n",
    "# Select features with correlation above the threshold\n",
    "high_corr_indices = np.where(correlations > correlation_threshold)[0]\n",
    "high_corr_features = X_train.columns[high_corr_indices]\n",
    "\n",
    "# Train and evaluate the model with high-correlation features\n",
    "dtrain_corr = xgb.DMatrix(X_train[high_corr_features].to_numpy(), label=y_train, weight=train_weights)\n",
    "dvalid_corr = xgb.DMatrix(X_valid[high_corr_features].to_numpy(), label=y_valid, weight=valid_weights)\n",
    "\n",
    "model_corr = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_corr,\n",
    "    num_boost_round=100,\n",
    "    evals=[(dtrain_corr, \"train\"), (dvalid_corr, \"valid\")],\n",
    "    early_stopping_rounds=10,\n",
    ")\n",
    "\n",
    "y_pred_corr = model_corr.predict(dvalid_corr)\n",
    "r2_score_corr = weighted_r2_score(y_valid.to_numpy(), y_pred_corr, valid_weights.to_numpy())\n",
    "print(f\"Low-Correlation Filtered Model Weighted R² Score: {r2_score_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0381660-6936-4afb-adf3-a703f7b7b666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=50,\n",
    "    nfold=5,  # Number of folds\n",
    "    metrics=\"rmse\",\n",
    "    early_stopping_rounds=10,\n",
    "    seed=42,\n",
    ")\n",
    "print(f\"Best RMSE: {cv_results['test-rmse-mean'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c177055-63e5-403c-bbd1-157d0b78e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_importance(model, importance_type='weight')  # Other types: 'gain', 'cover'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb1f43-d71f-4fdd-baec-9b8063ac3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Random Search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7, 9],\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBRegressor(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to test\n",
    "    cv=3,  # Cross-validation folds\n",
    "    verbose=1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params_random = random_search.best_params_\n",
    "\n",
    "# Step 2: Grid Search\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [5, 7],\n",
    "    \"n_estimators\": [100, 150],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # Cross-validation folds\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params_grid = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca878b-887e-4ae9-ad7c-e75b2f239861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, objective=\"reg:squarederror\", seed=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    rmse = np.sqrt(np.mean((preds - y_valid) ** 2))\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(f\"Best Parameters: {study.best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
