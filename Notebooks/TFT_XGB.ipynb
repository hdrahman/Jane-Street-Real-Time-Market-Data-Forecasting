{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd3de2c-af00-468a-8af8-32496b5485e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset (relative path from the notebook's location in Exploration folder)\n",
    "train_data = pd.read_parquet('../data/jane-street/train.parquet')\n",
    "\n",
    "# Display the first few rows\n",
    "#train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a85859-da4c-4a5d-ab4f-f553507faa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out the first 85 days\n",
    "train_data = train_data[train_data['date_id'] >= 85]\n",
    "\n",
    "# Drop rows that are completely empty\n",
    "train_data = train_data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd670c9-0f46-479a-b6e9-bb2df3430379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define columns to exclude\n",
    "exclude_columns = ['date_id', 'time_id', 'symbol_id', 'weight', 'partition_id'] + \\\n",
    "                  [col for col in train_data.columns if col.startswith('responder_')]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_columns = [col for col in train_data.columns if col not in exclude_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2b7308-351e-49fc-83bb-169a52f5ebc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interpolate missing values for numerical columns\n",
    "train_data[numerical_columns] = train_data[numerical_columns].interpolate(method='linear', axis=0)\n",
    "\n",
    "# Drop rows with any remaining NaN values\n",
    "train_data = train_data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e2c4e2-b660-4d21-a9b3-00d229530c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate IQR thresholds\n",
    "q1 = train_data[numerical_columns].quantile(0.25)\n",
    "q3 = train_data[numerical_columns].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate lower and upper bounds\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Clip outliers to bounds\n",
    "train_data[numerical_columns] = train_data[numerical_columns].clip(lower=lower_bound, upper=upper_bound, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1858f-e21d-439d-8b43-ce70ca7d90ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(train_data[numerical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e326937-1e1d-47bf-b325-2942c013ab34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''for col in numerical_columns:\n",
    "    length = train_data[col].size  # Size of the column\n",
    "    nan_count = train_data[col].isna().sum()  # Count of NaN values\n",
    "    print(f\"Column: {col}, Length: {length}, NaN Count: {nan_count}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370fe019-434b-4362-8a03-a84e3113c5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check for columns with NaN values\n",
    "nan_counts = train_data.isnull().sum()\n",
    "\n",
    "# Filter columns with NaN values\n",
    "columns_with_nan = nan_counts[nan_counts > 0]\n",
    "print(\"Columns with NaN values:\\n\", columns_with_nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91037e-b24a-4144-8be3-5329e6dee383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify constant columns (zero variance)\n",
    "constant_columns = [col for col in numerical_columns if train_data[col].nunique() == 1]\n",
    "print(\"Constant columns:\", constant_columns)\n",
    "\n",
    "# Drop constant columns\n",
    "train_data = train_data.drop(columns=constant_columns)\n",
    "numerical_columns = [col for col in numerical_columns if col not in constant_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eac4ee-fcdf-4d82-8fe4-b1c71a85a574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = train_data[numerical_columns].corr().abs()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "correlated_pairs = [(i, j) for i in corr_matrix.columns for j in corr_matrix.columns \n",
    "                    if i != j and corr_matrix.loc[i, j] > 0.9]\n",
    "\n",
    "print(\"Highly correlated pairs:\", correlated_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd83802-855c-45ee-9c02-5c53213e8219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop one column from each correlated pair\n",
    "columns_to_drop = set()\n",
    "for i, j in correlated_pairs:\n",
    "    if i not in columns_to_drop and j not in columns_to_drop:\n",
    "        columns_to_drop.add(j)\n",
    "\n",
    "train_data = train_data.drop(columns=list(columns_to_drop))\n",
    "numerical_columns = [col for col in numerical_columns if col not in columns_to_drop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4801a-76b5-473f-a1cc-269050793b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the cleaned data as Parquet\n",
    "train_data.to_parquet('../data/XGBoost.parquet', engine='pyarrow', compression='snappy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae2f50-94a7-497e-a634-54c45a236601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize scaler - TFT only\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale numerical columns\n",
    "train_data[numerical_columns] = scaler.fit_transform(train_data[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7e4b0-b4f7-405d-aeb5-c54f2427fc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Columns in saved data:\", train_data.columns)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
